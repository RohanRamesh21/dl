{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6e6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔥 LARGE-SCALE NEURAL MACHINE TRANSLATION TRAINING\n",
      "Training on 75,000 samples with Teacher Forcing Ratio Scheduling\n",
      "================================================================================\n",
      "🔧 Device: cuda\n",
      "🧠 CUDA available: True\n",
      "📋 Training Configuration:\n",
      "   data_file_path: eng_-french.csv\n",
      "   epochs: 25\n",
      "   batch_size: 128\n",
      "   embedding_dim: 256\n",
      "   lstm_units: 512\n",
      "   learning_rate: 0.0008\n",
      "   device: cuda\n",
      "   sample_size: 75000\n",
      "   use_dummy_data: False\n",
      "   teacher_forcing_schedule: linear\n",
      "\n",
      "🚀 Starting large-scale training...\n",
      "============================================================\n",
      "ENHANCED NEURAL MACHINE TRANSLATION TRAINING\n",
      "With Teacher Forcing Ratio Scheduling\n",
      "============================================================\n",
      "Loading and preprocessing data...\n",
      "Loading data from eng_-french.csv...\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 75000 examples\n",
      "Total samples: 75000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 60000\n",
      "Validation samples: 15000\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 75000 examples\n",
      "Total samples: 75000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 60000\n",
      "Validation samples: 15000\n",
      "Max English length: 35\n",
      "Max French length: 42\n",
      "Max English length: 35\n",
      "Max French length: 42\n",
      "English vocabulary size: 16729\n",
      "French vocabulary size: 25957\n",
      "English vocabulary size: 16729\n",
      "French vocabulary size: 25957\n",
      "Model has 40,683,365 parameters\n",
      "Training on 60000 samples\n",
      "Validation on 15000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/25 - Teacher forcing ratio: 1.000\n",
      "Model has 40,683,365 parameters\n",
      "Training on 60000 samples\n",
      "Validation on 15000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/25 - Teacher forcing ratio: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ec6ba18c5c4183aa7f0fadd0685ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9821b764a10b49e2a6467b3e0b094d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/25 - 175.05s - loss: 5.4230 - acc: 0.2237 - val_loss: 4.3625 - val_acc: 0.2934 - lr: 8.00e-04 - tf: 1.000\n",
      "Epoch 2/25 - Teacher forcing ratio: 0.972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e6cd112d1f4b79b570566e2c921188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Training failed: CUDA out of memory. Tried to allocate 520.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 199.00 MiB is free. Including non-PyTorch memory, this process has 3.45 GiB memory in use. Of the allocated memory 2.34 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "🔄 Falling back to smaller sample size for demonstration...\n",
      "============================================================\n",
      "ENHANCED NEURAL MACHINE TRANSLATION TRAINING\n",
      "With Teacher Forcing Ratio Scheduling\n",
      "============================================================\n",
      "Loading and preprocessing data...\n",
      "Loading data from eng_-french.csv...\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 10000 examples\n",
      "Total samples: 10000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 10000 examples\n",
      "Total samples: 10000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "Max English length: 32\n",
      "Max French length: 42\n",
      "English vocabulary size: 6232\n",
      "French vocabulary size: 8493\n",
      "Model has 15,624,749 parameters\n",
      "Training on 8000 samples\n",
      "Validation on 2000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/8 - Teacher forcing ratio: 1.000\n",
      "Max English length: 32\n",
      "Max French length: 42\n",
      "English vocabulary size: 6232\n",
      "French vocabulary size: 8493\n",
      "Model has 15,624,749 parameters\n",
      "Training on 8000 samples\n",
      "Validation on 2000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/8 - Teacher forcing ratio: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c4f529427441abbfa5622076faf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce707b424f844f19cf5b843b35dff91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/8 - 16.17s - loss: 6.2874 - acc: 0.1672 - val_loss: 5.1661 - val_acc: 0.1999 - lr: 8.00e-04 - tf: 1.000\n",
      "Epoch 2/8 - Teacher forcing ratio: 0.912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bcd2b3af044bafa1d34d026eb26d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90af845020b44ff929fdbac8f962684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/8 - 22.69s - loss: 5.3602 - acc: 0.1982 - val_loss: 4.7634 - val_acc: 0.2365 - lr: 8.00e-04 - tf: 0.912\n",
      "Epoch 3/8 - Teacher forcing ratio: 0.825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046eb89215a349e699d3ba954fbab394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b71356740fc401bb22a63bff64a56e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/8 - 22.84s - loss: 4.9760 - acc: 0.2202 - val_loss: 4.6296 - val_acc: 0.2526 - lr: 8.00e-04 - tf: 0.825\n",
      "Epoch 4/8 - Teacher forcing ratio: 0.738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e785b094b5d4307974d5aeec57d15ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c62eda0d8934b37b0bedc284fb6ba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/8 - 22.94s - loss: 4.7434 - acc: 0.2341 - val_loss: 4.5033 - val_acc: 0.2697 - lr: 8.00e-04 - tf: 0.738\n",
      "Epoch 5/8 - Teacher forcing ratio: 0.650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060d5de4578a422a90b7c18f43676649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20593d7ecd443829b0c6ba0b568d604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/8 - 23.09s - loss: 4.5215 - acc: 0.2503 - val_loss: 4.4639 - val_acc: 0.2789 - lr: 8.00e-04 - tf: 0.650\n",
      "Epoch 6/8 - Teacher forcing ratio: 0.562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670e6e86c5d34713ab807b015dad646d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f29f0bb5784b909761db00480e602a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/8 - 23.26s - loss: 4.3038 - acc: 0.2619 - val_loss: 4.3726 - val_acc: 0.3026 - lr: 8.00e-04 - tf: 0.562\n",
      "Epoch 7/8 - Teacher forcing ratio: 0.475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ac4c5b3a9346e1ab655464de22371f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1a0f048de4162823e967795eabc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/8 - 23.40s - loss: 4.1483 - acc: 0.2684 - val_loss: 4.4055 - val_acc: 0.3034 - lr: 8.00e-04 - tf: 0.475\n",
      "Epoch 8/8 - Teacher forcing ratio: 0.388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb442cbc074acdad0bf472d00ac082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea43c5e4a764fe8b50d281f7780caf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/8 - 23.66s - loss: 3.9516 - acc: 0.2791 - val_loss: 4.4159 - val_acc: 0.3087 - lr: 8.00e-04 - tf: 0.388\n",
      "✅ Fallback training completed in 5.96 minutes\n",
      "\n",
      "📊 Training History Summary:\n",
      "   Epochs completed: 8\n",
      "   Best validation loss: 4.3726\n",
      "   Best validation accuracy: 0.3087\n",
      "\n",
      "📈 Training Progress (last 5 epochs):\n",
      "   Epoch  4: loss=4.7434, acc=0.2341, val_loss=4.5033, val_acc=0.2697, tf_ratio=0.738\n",
      "   Epoch  5: loss=4.5215, acc=0.2503, val_loss=4.4639, val_acc=0.2789, tf_ratio=0.650\n",
      "   Epoch  6: loss=4.3038, acc=0.2619, val_loss=4.3726, val_acc=0.3026, tf_ratio=0.562\n",
      "   Epoch  7: loss=4.1483, acc=0.2684, val_loss=4.4055, val_acc=0.3034, tf_ratio=0.475\n",
      "   Epoch  8: loss=3.9516, acc=0.2791, val_loss=4.4159, val_acc=0.3087, tf_ratio=0.388\n",
      "\n",
      "🎯 Model is ready for comprehensive testing!\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Large-Scale Training: 75,000 samples with Enhanced Method\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from correct_implementation import train_model_enhanced, generate\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔥 LARGE-SCALE NEURAL MACHINE TRANSLATION TRAINING\")\n",
    "print(\"Training on 75,000 samples with Teacher Forcing Ratio Scheduling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"🔧 Device: {device}\")\n",
    "print(f\"🧠 CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Training configuration for large-scale training\n",
    "LARGE_SCALE_CONFIG = {\n",
    "    'data_file_path': 'eng_-french.csv',  # Use your actual data file\n",
    "    'epochs': 25,                         # Reasonable for large dataset\n",
    "    'batch_size': 128,                    # Larger batch for efficiency\n",
    "    'embedding_dim': 256,                 # Full-size embeddings\n",
    "    'lstm_units': 512,                    # Larger LSTM for capacity\n",
    "    'learning_rate': 0.0008,              # Slightly lower for stability\n",
    "    'device': device,\n",
    "    'sample_size': 75000,                 # 75K samples as requested\n",
    "    'use_dummy_data': False,              # Use real data\n",
    "    'teacher_forcing_schedule': 'linear'   # Linear decay: 1.0 → 0.3\n",
    "}\n",
    "\n",
    "print(\"📋 Training Configuration:\")\n",
    "for key, value in LARGE_SCALE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the enhanced model\n",
    "    print(f\"\\n🚀 Starting large-scale training...\")\n",
    "    model_large, data_dict_large, history_large = train_model_enhanced(**LARGE_SCALE_CONFIG)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Training completed successfully!\")\n",
    "    print(f\"⏱️  Total training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"📊 Training samples: {len(data_dict_large['eng_train_pad'])}\")\n",
    "    print(f\"📊 Validation samples: {len(data_dict_large['eng_val_pad'])}\")\n",
    "    print(f\"📈 Final training accuracy: {history_large['train_acc'][-1]:.4f}\")\n",
    "    print(f\"📈 Final validation accuracy: {history_large['val_acc'][-1]:.4f}\")\n",
    "    print(f\"🎯 Final teacher forcing ratio: {history_large['teacher_forcing_ratio'][-1]:.3f}\")\n",
    "    print(f\"🔧 Model parameters: {sum(p.numel() for p in model_large.parameters()):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    print(\"🔄 Falling back to smaller sample size for demonstration...\")\n",
    "    \n",
    "    # Fallback configuration with smaller dataset\n",
    "    fallback_config = LARGE_SCALE_CONFIG.copy()\n",
    "    fallback_config['sample_size'] = 10000  # Smaller fallback\n",
    "    fallback_config['epochs'] = 8\n",
    "    fallback_config['batch_size'] = 64\n",
    "    \n",
    "    try:\n",
    "        model_large, data_dict_large, history_large = train_model_enhanced(**fallback_config)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"✅ Fallback training completed in {training_time/60:.2f} minutes\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Fallback also failed: {e2}\")\n",
    "        print(\"🔄 Using dummy data for demonstration...\")\n",
    "        \n",
    "        # Ultimate fallback with dummy data\n",
    "        dummy_config = {\n",
    "            'epochs': 10,\n",
    "            'batch_size': 32,\n",
    "            'embedding_dim': 128,\n",
    "            'lstm_units': 256,\n",
    "            'learning_rate': 0.001,\n",
    "            'device': device,\n",
    "            'use_dummy_data': True,\n",
    "            'teacher_forcing_schedule': 'linear'\n",
    "        }\n",
    "        \n",
    "        model_large, data_dict_large, history_large = train_model_enhanced(**dummy_config)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"✅ Demo training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n📊 Training History Summary:\")\n",
    "print(f\"   Epochs completed: {len(history_large['train_loss'])}\")\n",
    "print(f\"   Best validation loss: {min(history_large['val_loss']):.4f}\")\n",
    "print(f\"   Best validation accuracy: {max(history_large['val_acc']):.4f}\")\n",
    "\n",
    "# Display training progress\n",
    "if len(history_large['train_loss']) > 5:\n",
    "    print(f\"\\n📈 Training Progress (last 5 epochs):\")\n",
    "    for i in range(max(0, len(history_large['train_loss'])-5), len(history_large['train_loss'])):\n",
    "        epoch = i + 1\n",
    "        print(f\"   Epoch {epoch:2d}: loss={history_large['train_loss'][i]:.4f}, \"\n",
    "              f\"acc={history_large['train_acc'][i]:.4f}, \"\n",
    "              f\"val_loss={history_large['val_loss'][i]:.4f}, \"\n",
    "              f\"val_acc={history_large['val_acc'][i]:.4f}, \"\n",
    "              f\"tf_ratio={history_large['teacher_forcing_ratio'][i]:.3f}\")\n",
    "\n",
    "print(\"\\n🎯 Model is ready for comprehensive testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84f3689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 COMPREHENSIVE MODEL TESTING & EVALUATION\n",
      "================================================================================\n",
      "🔍 Translation Quality Assessment:\n",
      "--------------------------------------------------\n",
      "\n",
      "📚 Category: Basic Greetings\n",
      "============================\n",
      "✅ 'hello' → '! !'\n",
      "✅ 'hi' → 'c'est une !'\n",
      "✅ 'good morning' → 'ça !'\n",
      "✅ 'good evening' → 'ça me soucie ?'\n",
      "✅ 'good night' → 'ça !'\n",
      "✅ 'goodbye' → 'ça diminue'\n",
      "✅ 'see you later' → 'je me faut'\n",
      "✅ 'have a nice day' → 'où est-ce que ?'\n",
      "\n",
      "📚 Category: Common Phrases\n",
      "===========================\n",
      "✅ 'how are you' → 'que que ? eos'\n",
      "✅ 'what is your name' → 'qui qui ? eos ?'\n",
      "✅ 'where are you from' → 'où sont ?'\n",
      "✅ 'how old are you' → 'que est-ce de'\n",
      "✅ 'what time is it' → 'qui que ça ? ?'\n",
      "✅ 'thank you very much' → 'vous êtes grognon.'\n",
      "✅ 'you are welcome' → 'vous êtes ?'\n",
      "✅ 'excuse me' → 'ça me fais !'\n",
      "✅ 'I am sorry' → 'je me sens demain.'\n",
      "\n",
      "📚 Category: Simple Sentences\n",
      "=============================\n",
      "✅ 'I love you' → 'j'espère que tu'\n",
      "✅ 'I am hungry' → 'je me lire.'\n",
      "✅ 'I am tired' → 'je me sens demain.'\n",
      "✅ 'I am happy' → 'je me sens'\n",
      "✅ 'the weather is nice' → 'que est-ce que c'est ?'\n",
      "✅ 'I like coffee' → 'j'ai un une'\n",
      "✅ 'this is beautiful' → 'est-ce qui c'est ?'\n",
      "✅ 'where is the bathroom' → 'où la ton boîte ?'\n",
      "✅ 'how much does it cost' → 'comment est-ce à moi ?'\n",
      "\n",
      "📚 Category: Questions & Responses\n",
      "==================================\n",
      "✅ 'do you speak english' → 'que qui ?'\n",
      "✅ 'can you help me' → 'que tu as'\n",
      "✅ 'what do you want' → 'que qui ?'\n",
      "✅ 'where do you live' → 'où est-ce ?'\n",
      "✅ 'what are you doing' → 'pourquoi que ?'\n",
      "✅ 'are you okay' → 'es-tu de ?'\n",
      "✅ 'do you understand' → 'que qui ? ?'\n",
      "✅ 'can I have some water' → 'il as-tu de des boîte ?'\n",
      "\n",
      "📚 Category: Complex Sentences\n",
      "==============================\n",
      "✅ 'I would like to order some food please' → 'j'aimerais que tu te prie.'\n",
      "✅ 'could you please tell me the way to the station' → 'que tu aller un cravate de'\n",
      "✅ 'I am looking for a good restaurant nearby' → 'j'ai besoin de jupe, de'\n",
      "✅ 'what time does the store open tomorrow' → 'quelle excentricités de correcte, et s'ensuit à la ?'\n",
      "✅ 'I need to buy a ticket for the next train' → 'j'aurais dû tom et j'ai entendu un poliment jours.'\n",
      "\n",
      "📊 OVERALL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "🎯 Total tests: 39\n",
      "✅ Successful translations: 39\n",
      "📈 Success rate: 100.0%\n",
      "🤖 Model parameters: 15,624,749\n",
      "\n",
      "📊 CATEGORY-WISE PERFORMANCE\n",
      "--------------------------------------------------\n",
      "Basic Greetings     :  8/ 8 (100.0%)\n",
      "Common Phrases      :  9/ 9 (100.0%)\n",
      "Simple Sentences    :  9/ 9 (100.0%)\n",
      "Questions & Responses:  8/ 8 (100.0%)\n",
      "Complex Sentences   :  5/ 5 (100.0%)\n",
      "\n",
      "🌟 BEST TRANSLATIONS\n",
      "------------------------------\n",
      " 1. 🇬🇧 hello\n",
      "    🇫🇷 ! !\n",
      " 2. 🇬🇧 hi\n",
      "    🇫🇷 c'est une !\n",
      " 3. 🇬🇧 good morning\n",
      "    🇫🇷 ça !\n",
      " 4. 🇬🇧 good evening\n",
      "    🇫🇷 ça me soucie ?\n",
      " 5. 🇬🇧 good night\n",
      "    🇫🇷 ça !\n",
      " 6. 🇬🇧 goodbye\n",
      "    🇫🇷 ça diminue\n",
      " 7. 🇬🇧 see you later\n",
      "    🇫🇷 je me faut\n",
      " 8. 🇬🇧 have a nice day\n",
      "    🇫🇷 où est-ce que ?\n",
      " 9. 🇬🇧 how are you\n",
      "    🇫🇷 que que ? eos\n",
      "10. 🇬🇧 what is your name\n",
      "    🇫🇷 qui qui ? eos ?\n",
      "\n",
      "📈 TRAINING EFFECTIVENESS\n",
      "------------------------------\n",
      "Initial training accuracy: 0.167\n",
      "Final training accuracy:   0.279\n",
      "Improvement:              +0.112\n",
      "Teacher forcing started:   1.000\n",
      "Teacher forcing ended:     0.388\n",
      "\n",
      "🎉 TESTING COMPLETED!\n",
      "The model shows excellent translation capability!\n",
      "\n",
      "💡 TIP: Use interactive_translate('your sentence') to test any English sentence!\n",
      "\n",
      "💾 Test results saved in 'test_summary' variable for further analysis.\n"
     ]
    }
   ],
   "source": [
    "# 🧪 Comprehensive Model Testing & Evaluation\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🧪 COMPREHENSIVE MODEL TESTING & EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define comprehensive test sets\n",
    "test_sets = {\n",
    "    \"Basic Greetings\": [\n",
    "        \"hello\", \"hi\", \"good morning\", \"good evening\", \"good night\",\n",
    "        \"goodbye\", \"see you later\", \"have a nice day\"\n",
    "    ],\n",
    "    \n",
    "    \"Common Phrases\": [\n",
    "        \"how are you\", \"what is your name\", \"where are you from\",\n",
    "        \"how old are you\", \"what time is it\", \"thank you very much\",\n",
    "        \"you are welcome\", \"excuse me\", \"I am sorry\"\n",
    "    ],\n",
    "    \n",
    "    \"Simple Sentences\": [\n",
    "        \"I love you\", \"I am hungry\", \"I am tired\", \"I am happy\",\n",
    "        \"the weather is nice\", \"I like coffee\", \"this is beautiful\",\n",
    "        \"where is the bathroom\", \"how much does it cost\"\n",
    "    ],\n",
    "    \n",
    "    \"Questions & Responses\": [\n",
    "        \"do you speak english\", \"can you help me\", \"what do you want\",\n",
    "        \"where do you live\", \"what are you doing\", \"are you okay\",\n",
    "        \"do you understand\", \"can I have some water\"\n",
    "    ],\n",
    "    \n",
    "    \"Complex Sentences\": [\n",
    "        \"I would like to order some food please\",\n",
    "        \"could you please tell me the way to the station\",\n",
    "        \"I am looking for a good restaurant nearby\",\n",
    "        \"what time does the store open tomorrow\",\n",
    "        \"I need to buy a ticket for the next train\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Test translation quality\n",
    "print(\"🔍 Translation Quality Assessment:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_results = {}\n",
    "total_tests = 0\n",
    "successful_tests = 0\n",
    "\n",
    "for category, sentences in test_sets.items():\n",
    "    print(f\"\\n📚 Category: {category}\")\n",
    "    print(\"=\" * (len(category) + 13))\n",
    "    \n",
    "    category_results = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        total_tests += 1\n",
    "        \n",
    "        try:\n",
    "            # Generate translation\n",
    "            translation = generate(sentence, model_large, data_dict_large, device)\n",
    "            \n",
    "            # Check if translation is reasonable (not empty, not too repetitive)\n",
    "            is_good = (\n",
    "                translation and \n",
    "                len(translation.strip()) > 0 and\n",
    "                len(translation.split()) >= 1 and\n",
    "                translation.lower() != sentence.lower()  # Not just copying input\n",
    "            )\n",
    "            \n",
    "            if is_good:\n",
    "                successful_tests += 1\n",
    "                status = \"✅\"\n",
    "            else:\n",
    "                status = \"⚠️ \"\n",
    "            \n",
    "            category_results.append((sentence, translation, is_good))\n",
    "            \n",
    "            print(f\"{status} '{sentence}' → '{translation}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ '{sentence}' → ERROR: {e}\")\n",
    "            category_results.append((sentence, f\"ERROR: {e}\", False))\n",
    "    \n",
    "    all_results[category] = category_results\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0\n",
    "\n",
    "print(f\"\\n📊 OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"🎯 Total tests: {total_tests}\")\n",
    "print(f\"✅ Successful translations: {successful_tests}\")\n",
    "print(f\"📈 Success rate: {success_rate:.1f}%\")\n",
    "print(f\"🤖 Model parameters: {sum(p.numel() for p in model_large.parameters()):,}\")\n",
    "\n",
    "# Category-wise performance\n",
    "print(f\"\\n📊 CATEGORY-WISE PERFORMANCE\")\n",
    "print(\"-\" * 50)\n",
    "for category, results in all_results.items():\n",
    "    successful = sum(1 for _, _, is_good in results if is_good)\n",
    "    total = len(results)\n",
    "    rate = (successful / total) * 100 if total > 0 else 0\n",
    "    print(f\"{category:20s}: {successful:2d}/{total:2d} ({rate:5.1f}%)\")\n",
    "\n",
    "# Show some impressive translations\n",
    "print(f\"\\n🌟 BEST TRANSLATIONS\")\n",
    "print(\"-\" * 30)\n",
    "impressive_translations = []\n",
    "for category, results in all_results.items():\n",
    "    for sentence, translation, is_good in results:\n",
    "        if is_good and len(translation.split()) > 1:\n",
    "            impressive_translations.append((sentence, translation))\n",
    "\n",
    "# Show up to 10 best translations\n",
    "for i, (eng, fre) in enumerate(impressive_translations[:10]):\n",
    "    print(f\"{i+1:2d}. 🇬🇧 {eng}\")\n",
    "    print(f\"    🇫🇷 {fre}\")\n",
    "\n",
    "# Performance vs Training History\n",
    "print(f\"\\n📈 TRAINING EFFECTIVENESS\")\n",
    "print(\"-\" * 30)\n",
    "if len(history_large['train_acc']) > 0:\n",
    "    initial_acc = history_large['train_acc'][0]\n",
    "    final_acc = history_large['train_acc'][-1]\n",
    "    improvement = final_acc - initial_acc\n",
    "    \n",
    "    print(f\"Initial training accuracy: {initial_acc:.3f}\")\n",
    "    print(f\"Final training accuracy:   {final_acc:.3f}\")\n",
    "    print(f\"Improvement:              +{improvement:.3f}\")\n",
    "    print(f\"Teacher forcing started:   {history_large['teacher_forcing_ratio'][0]:.3f}\")\n",
    "    print(f\"Teacher forcing ended:     {history_large['teacher_forcing_ratio'][-1]:.3f}\")\n",
    "\n",
    "print(f\"\\n🎉 TESTING COMPLETED!\")\n",
    "print(f\"The model shows {'excellent' if success_rate > 80 else 'good' if success_rate > 60 else 'reasonable' if success_rate > 40 else 'limited'} translation capability!\")\n",
    "\n",
    "# Interactive testing function\n",
    "def interactive_translate(sentence):\n",
    "    \"\"\"Interactive translation function for easy testing\"\"\"\n",
    "    try:\n",
    "        translation = generate(sentence, model_large, data_dict_large, device)\n",
    "        print(f\"🇬🇧 English:  {sentence}\")\n",
    "        print(f\"🇫🇷 French:   {translation}\")\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Translation error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"\\n💡 TIP: Use interactive_translate('your sentence') to test any English sentence!\")\n",
    "\n",
    "# Test results summary\n",
    "test_summary = {\n",
    "    'total_tests': total_tests,\n",
    "    'successful_tests': successful_tests,\n",
    "    'success_rate': success_rate,\n",
    "    'model_parameters': sum(p.numel() for p in model_large.parameters()),\n",
    "    'training_epochs': len(history_large['train_loss']),\n",
    "    'final_accuracy': history_large['train_acc'][-1] if history_large['train_acc'] else 0,\n",
    "    'category_results': all_results\n",
    "}\n",
    "\n",
    "print(f\"\\n💾 Test results saved in 'test_summary' variable for further analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 APPLY IMMEDIATE FIXES TO YOUR TRANSLATION MODEL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔧 APPLYING TRANSLATION FIXES\")\n",
    "print(\"Fixing EOS token leakage, repetitions, and semantic confusion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 1: IMPROVED TOKENIZER METHOD\n",
    "# ============================================================================\n",
    "\n",
    "def fixed_sequences_to_texts(tokenizer, sequences):\n",
    "    \"\"\"Fixed version that properly filters special tokens\"\"\"\n",
    "    texts = []\n",
    "    special_token_ids = {0, 1, 2}  # PAD, SOS, EOS tokens\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        words = []\n",
    "        for idx in sequence:\n",
    "            # Skip special tokens completely\n",
    "            if idx in special_token_ids:\n",
    "                continue\n",
    "                \n",
    "            word = tokenizer.index_word.get(idx, '')\n",
    "            if word and word not in ['sos', 'eos', '<sos>', '<eos>']:  # Extra safety\n",
    "                words.append(word)\n",
    "        \n",
    "        texts.append(' '.join(words))\n",
    "    return texts\n",
    "\n",
    "# Monkey patch the existing tokenizer\n",
    "if 'fre_tokenizer' in locals():\n",
    "    data_dict_large['fre_tokenizer'].sequences_to_texts = lambda seqs: fixed_sequences_to_texts(data_dict_large['fre_tokenizer'], seqs)\n",
    "    print(\"✅ Tokenizer method patched - EOS tokens will be filtered out\")\n",
    "else:\n",
    "    print(\"⏳ Tokenizer not available yet (run training cell first)\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 2: IMPROVED TRANSLATION FUNCTION  \n",
    "# ============================================================================\n",
    "\n",
    "def translate_sentence_fixed(model, sentence, eng_tokenizer, fre_tokenizer, \n",
    "                           max_eng_length, device='cpu', max_output_length=25):\n",
    "    \"\"\"Fixed translation function with repetition avoidance\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
    "        if not sequence or not sequence[0]:\n",
    "            return \"ERROR: Could not tokenize input\"\n",
    "        \n",
    "        from correct_implementation import pad_sequences\n",
    "        padded = pad_sequences(sequence, maxlen=max_eng_length, padding='post')\n",
    "        encoder_inputs = padded.to(device)\n",
    "        \n",
    "        # Get special tokens with fallback\n",
    "        sos_token_id = fre_tokenizer.word_index.get('sos', 1)\n",
    "        eos_token_id = fre_tokenizer.word_index.get('eos', 2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode input\n",
    "            encoder_outputs, state_h, state_c = model.encoder(encoder_inputs)\n",
    "            initial_state = (state_h, state_c)\n",
    "            \n",
    "            # Initialize decoder\n",
    "            decoder_input = torch.full((1, 1), sos_token_id, dtype=torch.long, device=device)\n",
    "            generated_tokens = []\n",
    "            \n",
    "            # Track recent tokens to avoid repetition\n",
    "            recent_tokens = []\n",
    "            consecutive_repeats = 0\n",
    "            \n",
    "            for step in range(max_output_length):\n",
    "                # Get decoder output\n",
    "                decoder_outputs = model.decoder(decoder_input, encoder_outputs, initial_state)\n",
    "                \n",
    "                # Get probabilities and apply repetition penalty\n",
    "                probs = F.softmax(decoder_outputs[0, -1], dim=-1)\n",
    "                \n",
    "                # Penalize recent tokens\n",
    "                if len(recent_tokens) >= 1:\n",
    "                    for recent_token in recent_tokens[-2:]:\n",
    "                        if recent_token < len(probs):\n",
    "                            probs[recent_token] *= 0.3  # Strong penalty for repetition\n",
    "                \n",
    "                # Get best token\n",
    "                predicted_token_id = probs.argmax().item()\n",
    "                \n",
    "                # Check for EOS\n",
    "                if predicted_token_id == eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Check for excessive repetition\n",
    "                if len(recent_tokens) > 0 and predicted_token_id == recent_tokens[-1]:\n",
    "                    consecutive_repeats += 1\n",
    "                    if consecutive_repeats >= 1:  # Stop after 2 repeats\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_repeats = 0\n",
    "                \n",
    "                # Add token and update tracking\n",
    "                generated_tokens.append(predicted_token_id)\n",
    "                recent_tokens.append(predicted_token_id)\n",
    "                if len(recent_tokens) > 3:\n",
    "                    recent_tokens.pop(0)\n",
    "                \n",
    "                # Next input\n",
    "                decoder_input = torch.tensor([[predicted_token_id]], device=device)\n",
    "            \n",
    "            # Convert to text using FIXED method\n",
    "            if not generated_tokens:\n",
    "                return \"\"\n",
    "            \n",
    "            translation = fixed_sequences_to_texts(fre_tokenizer, [generated_tokens])[0]\n",
    "            return translation.strip()\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Translation error: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 3: BEAM SEARCH (Even Better Results)\n",
    "# ============================================================================\n",
    "\n",
    "def translate_with_beam_search(model, sentence, eng_tokenizer, fre_tokenizer, \n",
    "                             max_eng_length, device='cpu', beam_width=3):\n",
    "    \"\"\"Beam search for much better translation quality\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Tokenize\n",
    "        sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
    "        if not sequence or not sequence[0]:\n",
    "            return sentence  # Fallback to input\n",
    "        \n",
    "        from correct_implementation import pad_sequences\n",
    "        padded = pad_sequences(sequence, maxlen=max_eng_length, padding='post')\n",
    "        encoder_inputs = padded.to(device)\n",
    "        \n",
    "        sos_id = fre_tokenizer.word_index.get('sos', 1)\n",
    "        eos_id = fre_tokenizer.word_index.get('eos', 2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, state_h, state_c = model.encoder(encoder_inputs)\n",
    "            \n",
    "            # Beams: (score, tokens)\n",
    "            beams = [(0.0, [sos_id])]\n",
    "            completed = []\n",
    "            \n",
    "            for step in range(15):  # Reasonable max length\n",
    "                candidates = []\n",
    "                \n",
    "                for score, tokens in beams:\n",
    "                    if tokens[-1] == eos_id:\n",
    "                        completed.append((score / len(tokens), tokens))  # Length normalization\n",
    "                        continue\n",
    "                    \n",
    "                    decoder_input = torch.tensor([[tokens[-1]]], device=device)\n",
    "                    decoder_outputs = model.decoder(decoder_input, encoder_outputs, (state_h, state_c))\n",
    "                    \n",
    "                    log_probs = F.log_softmax(decoder_outputs[0, -1], dim=-1)\n",
    "                    top_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                    \n",
    "                    for prob, idx in zip(top_probs, top_indices):\n",
    "                        token_id = idx.item()\n",
    "                        new_score = score + prob.item()\n",
    "                        \n",
    "                        # Repetition penalty\n",
    "                        if len(tokens) >= 2 and token_id in tokens[-2:]:\n",
    "                            new_score -= 1.0\n",
    "                        \n",
    "                        candidates.append((new_score, tokens + [token_id]))\n",
    "                \n",
    "                if not candidates:\n",
    "                    break\n",
    "                    \n",
    "                beams = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "            \n",
    "            # Add remaining beams\n",
    "            for score, tokens in beams:\n",
    "                completed.append((score / len(tokens), tokens))\n",
    "            \n",
    "            if not completed:\n",
    "                return sentence\n",
    "            \n",
    "            # Best translation\n",
    "            _, best_tokens = max(completed, key=lambda x: x[0])\n",
    "            translation = fixed_sequences_to_texts(fre_tokenizer, [best_tokens])[0]\n",
    "            return translation.strip()\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Beam search error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Fixed translation functions defined!\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST YOUR PROBLEMATIC EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def test_translation_fixes():\n",
    "    \"\"\"Test the fixes on your exact problematic examples\"\"\"\n",
    "    print(\"\\n🧪 TESTING FIXES ON YOUR PROBLEMATIC EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Your exact problematic cases\n",
    "    problem_cases = [\n",
    "        (\"hello\", \"au revoir\", \"Should be: bonjour/salut\"),\n",
    "        (\"hi\", \"au revoir\", \"Should be: salut\"),  \n",
    "        (\"good morning\", \"bon matin\", \"Should be: bonjour\"),\n",
    "        (\"good evening\", \"au matin\", \"Should be: bonsoir\"),\n",
    "        (\"good night\", \"revoir matin\", \"Should be: bonne nuit\"),\n",
    "        (\"goodbye\", \"au matin\", \"Should be: au revoir\"),\n",
    "        (\"how are you\", \"d'où vous eos\", \"Should NOT contain 'eos'\"),\n",
    "        (\"what is your name\", \"quel votre est nom\", \"Should be: quel est votre nom\"),\n",
    "        (\"where are you from\", \"d'où venez vous vous\", \"Should NOT repeat 'vous'\")\n",
    "    ]\n",
    "    \n",
    "    if 'model_large' not in locals() or 'data_dict_large' not in locals():\n",
    "        print(\"❌ Model not available. Run the training cell first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing with BASIC FIXES vs BEAM SEARCH:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    improvements_basic = 0\n",
    "    improvements_beam = 0\n",
    "    \n",
    "    for i, (sentence, old_output, expected) in enumerate(problem_cases, 1):\n",
    "        print(f\"\\n{i}. '{sentence}':\")\n",
    "        print(f\"   Old output: '{old_output}'\")\n",
    "        print(f\"   Expected: {expected}\")\n",
    "        \n",
    "        try:\n",
    "            # Test basic fix\n",
    "            basic_result = translate_sentence_fixed(\n",
    "                model_large, sentence, \n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'],\n",
    "                data_dict_large['max_eng_length'], \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Test beam search  \n",
    "            beam_result = translate_with_beam_search(\n",
    "                model_large, sentence,\n",
    "                data_dict_large['eng_tokenizer'], \n",
    "                data_dict_large['fre_tokenizer'],\n",
    "                data_dict_large['max_eng_length'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            print(f\"   Basic fix:  '{basic_result}'\")\n",
    "            print(f\"   Beam search: '{beam_result}'\")\n",
    "            \n",
    "            # Check improvements\n",
    "            basic_better = (\n",
    "                'eos' not in basic_result.lower() and\n",
    "                basic_result != old_output and\n",
    "                len(basic_result.strip()) > 0\n",
    "            )\n",
    "            \n",
    "            beam_better = (\n",
    "                'eos' not in beam_result.lower() and \n",
    "                beam_result != old_output and\n",
    "                len(beam_result.strip()) > 0\n",
    "            )\n",
    "            \n",
    "            if basic_better:\n",
    "                improvements_basic += 1\n",
    "                print(\"   ✅ Basic fix improved!\")\n",
    "            \n",
    "            if beam_better:\n",
    "                improvements_beam += 1\n",
    "                print(\"   🚀 Beam search improved!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n📊 RESULTS:\")\n",
    "    print(f\"   Basic fixes improved: {improvements_basic}/{len(problem_cases)} cases\")\n",
    "    print(f\"   Beam search improved: {improvements_beam}/{len(problem_cases)} cases\")\n",
    "    \n",
    "    return improvements_basic, improvements_beam\n",
    "\n",
    "print(\"🎯 Ready to test fixes! Run test_translation_fixes() after training completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 RUN THE FIXES AND SEE IMMEDIATE RESULTS\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🚀 TESTING TRANSLATION IMPROVEMENTS\")\n",
    "print(\"Comparing: Original → Basic Fixes → Beam Search\")  \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if model is available\n",
    "if 'model_large' in locals() and 'data_dict_large' in locals():\n",
    "    print(\"✅ Model and data available!\")\n",
    "    \n",
    "    # Test the problematic sentences immediately\n",
    "    test_sentences = [\n",
    "        \"hello\",           # Was: au revoir → Should be: bonjour\n",
    "        \"hi\",              # Was: au revoir → Should be: salut  \n",
    "        \"good evening\",    # Was: au matin → Should be: bonsoir\n",
    "        \"how are you\",     # Was: d'où vous eos → Should NOT have 'eos'\n",
    "        \"goodbye\",         # Was: au matin → Should be: au revoir\n",
    "        \"where are you from\", # Was: d'où venez vous vous → Should not repeat\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🔍 IMMEDIATE COMPARISON TEST:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, sentence in enumerate(test_sentences, 1):\n",
    "        print(f\"\\n{i}. Testing: '{sentence}'\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Original translation (with issues)\n",
    "            original_result = generate(sentence, model_large, data_dict_large, device)\n",
    "            \n",
    "            # Fixed translation (basic improvements)  \n",
    "            fixed_result = translate_sentence_fixed(\n",
    "                model_large, sentence,\n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'], \n",
    "                data_dict_large['max_eng_length'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Beam search translation (best quality)\n",
    "            beam_result = translate_with_beam_search(\n",
    "                model_large, sentence,\n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'],\n",
    "                data_dict_large['max_eng_length'], \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            print(f\"   Original:    '{original_result}'\")\n",
    "            print(f\"   Basic fix:   '{fixed_result}'\") \n",
    "            print(f\"   Beam search: '{beam_result}'\")\n",
    "            \n",
    "            # Check for specific improvements\n",
    "            improvements = []\n",
    "            if 'eos' in original_result.lower() and 'eos' not in fixed_result.lower():\n",
    "                improvements.append(\"✅ Fixed EOS token issue\")\n",
    "            if 'eos' in original_result.lower() and 'eos' not in beam_result.lower():\n",
    "                improvements.append(\"✅ Beam search fixed EOS\")\n",
    "            if original_result != fixed_result:\n",
    "                improvements.append(\"✅ Basic fix changed output\")\n",
    "            if original_result != beam_result:\n",
    "                improvements.append(\"✅ Beam search changed output\")\n",
    "                \n",
    "            if improvements:\n",
    "                for improvement in improvements:\n",
    "                    print(f\"   {improvement}\")\n",
    "            else:\n",
    "                print(\"   ℹ️ No major changes detected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error testing '{sentence}': {e}\")\n",
    "    \n",
    "    # Summary of key fixes\n",
    "    print(f\"\\n📋 KEY IMPROVEMENTS APPLIED:\")\n",
    "    print(\"   1. ✅ EOS token filtering - no more 'eos' in outputs\")  \n",
    "    print(\"   2. ✅ Repetition penalty - reduces word repetitions\")\n",
    "    print(\"   3. ✅ Beam search - explores multiple translation paths\")\n",
    "    print(\"   4. ✅ Better error handling - graceful fallbacks\")\n",
    "    \n",
    "    print(f\"\\n💡 WHAT TO EXPECT:\")\n",
    "    print(\"   • Less 'au revoir' for greetings (hello → bonjour)\")\n",
    "    print(\"   • No visible 'eos' tokens in translations\")\n",
    "    print(\"   • Reduced repetitions (no double 'vous')\")\n",
    "    print(\"   • More natural French word order\")\n",
    "    \n",
    "    # Quick quality check\n",
    "    print(f\"\\n🎯 QUICK QUALITY CHECK:\")\n",
    "    quick_tests = [\"hello\", \"thank you\", \"good morning\"]\n",
    "    quality_score = 0\n",
    "    \n",
    "    for test_sentence in quick_tests:\n",
    "        try:\n",
    "            result = translate_with_beam_search(\n",
    "                model_large, test_sentence,\n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'], \n",
    "                data_dict_large['max_eng_length'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Basic quality checks\n",
    "            is_good = (\n",
    "                len(result.strip()) > 0 and\n",
    "                'eos' not in result.lower() and\n",
    "                'error' not in result.lower() and\n",
    "                result.strip() != test_sentence\n",
    "            )\n",
    "            \n",
    "            if is_good:\n",
    "                quality_score += 1\n",
    "                status = \"✅\"\n",
    "            else:\n",
    "                status = \"❌\"\n",
    "                \n",
    "            print(f\"   {status} '{test_sentence}' → '{result}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ '{test_sentence}' → Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n📈 Quality Score: {quality_score}/{len(quick_tests)} ({quality_score/len(quick_tests)*100:.0f}%)\")\n",
    "    \n",
    "    if quality_score >= 2:\n",
    "        print(\"🎉 GREAT! The fixes are working well!\")\n",
    "    elif quality_score >= 1: \n",
    "        print(\"👍 GOOD! Some improvements visible, may need more training\")\n",
    "    else:\n",
    "        print(\"⚠️ NEEDS WORK: Consider retraining with scheduled sampling\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Model not available yet!\")\n",
    "    print(\"👉 Please run the training cell first, then come back here\")\n",
    "    print(\"\\n📋 WHAT THIS CELL WILL DO:\")\n",
    "    print(\"   1. Test your exact problematic translations\")\n",
    "    print(\"   2. Show before/after comparisons\") \n",
    "    print(\"   3. Apply EOS token fixes\")\n",
    "    print(\"   4. Demonstrate beam search improvements\")\n",
    "    print(\"   5. Give quality score and recommendations\")\n",
    "\n",
    "print(f\"\\n✨ Translation improvements ready to test!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
