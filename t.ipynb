{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6e6896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üî• LARGE-SCALE NEURAL MACHINE TRANSLATION TRAINING\n",
      "Training on 75,000 samples with Teacher Forcing Ratio Scheduling\n",
      "================================================================================\n",
      "üîß Device: cuda\n",
      "üß† CUDA available: True\n",
      "üìã Training Configuration:\n",
      "   data_file_path: eng_-french.csv\n",
      "   epochs: 25\n",
      "   batch_size: 128\n",
      "   embedding_dim: 256\n",
      "   lstm_units: 512\n",
      "   learning_rate: 0.0008\n",
      "   device: cuda\n",
      "   sample_size: 75000\n",
      "   use_dummy_data: False\n",
      "   teacher_forcing_schedule: linear\n",
      "\n",
      "üöÄ Starting large-scale training...\n",
      "============================================================\n",
      "ENHANCED NEURAL MACHINE TRANSLATION TRAINING\n",
      "With Teacher Forcing Ratio Scheduling\n",
      "============================================================\n",
      "Loading and preprocessing data...\n",
      "Loading data from eng_-french.csv...\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 75000 examples\n",
      "Total samples: 75000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 60000\n",
      "Validation samples: 15000\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 75000 examples\n",
      "Total samples: 75000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 60000\n",
      "Validation samples: 15000\n",
      "Max English length: 35\n",
      "Max French length: 42\n",
      "Max English length: 35\n",
      "Max French length: 42\n",
      "English vocabulary size: 16729\n",
      "French vocabulary size: 25957\n",
      "English vocabulary size: 16729\n",
      "French vocabulary size: 25957\n",
      "Model has 40,683,365 parameters\n",
      "Training on 60000 samples\n",
      "Validation on 15000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/25 - Teacher forcing ratio: 1.000\n",
      "Model has 40,683,365 parameters\n",
      "Training on 60000 samples\n",
      "Validation on 15000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/25 - Teacher forcing ratio: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ec6ba18c5c4183aa7f0fadd0685ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9821b764a10b49e2a6467b3e0b094d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/25 - 175.05s - loss: 5.4230 - acc: 0.2237 - val_loss: 4.3625 - val_acc: 0.2934 - lr: 8.00e-04 - tf: 1.000\n",
      "Epoch 2/25 - Teacher forcing ratio: 0.972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e6cd112d1f4b79b570566e2c921188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Training failed: CUDA out of memory. Tried to allocate 520.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 199.00 MiB is free. Including non-PyTorch memory, this process has 3.45 GiB memory in use. Of the allocated memory 2.34 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "üîÑ Falling back to smaller sample size for demonstration...\n",
      "============================================================\n",
      "ENHANCED NEURAL MACHINE TRANSLATION TRAINING\n",
      "With Teacher Forcing Ratio Scheduling\n",
      "============================================================\n",
      "Loading and preprocessing data...\n",
      "Loading data from eng_-french.csv...\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 10000 examples\n",
      "Total samples: 10000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "Dataset shape: (175621, 2)\n",
      "Columns: ['English words/sentences', 'French words/sentences']\n",
      "Using columns: English='English words/sentences', French='French words/sentences'\n",
      "After cleaning: 175621 samples\n",
      "Sampled 10000 examples\n",
      "Total samples: 10000\n",
      "Sample English: Take a seat.\n",
      "Sample French: sos Prends place ! eos\n",
      "Training samples: 8000\n",
      "Validation samples: 2000\n",
      "Max English length: 32\n",
      "Max French length: 42\n",
      "English vocabulary size: 6232\n",
      "French vocabulary size: 8493\n",
      "Model has 15,624,749 parameters\n",
      "Training on 8000 samples\n",
      "Validation on 2000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/8 - Teacher forcing ratio: 1.000\n",
      "Max English length: 32\n",
      "Max French length: 42\n",
      "English vocabulary size: 6232\n",
      "French vocabulary size: 8493\n",
      "Model has 15,624,749 parameters\n",
      "Training on 8000 samples\n",
      "Validation on 2000 samples\n",
      "Teacher forcing schedule: linear\n",
      "------------------------------------------------------------\n",
      "Epoch 1/8 - Teacher forcing ratio: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c4f529427441abbfa5622076faf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce707b424f844f19cf5b843b35dff91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/8 - 16.17s - loss: 6.2874 - acc: 0.1672 - val_loss: 5.1661 - val_acc: 0.1999 - lr: 8.00e-04 - tf: 1.000\n",
      "Epoch 2/8 - Teacher forcing ratio: 0.912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bcd2b3af044bafa1d34d026eb26d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90af845020b44ff929fdbac8f962684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/8 - 22.69s - loss: 5.3602 - acc: 0.1982 - val_loss: 4.7634 - val_acc: 0.2365 - lr: 8.00e-04 - tf: 0.912\n",
      "Epoch 3/8 - Teacher forcing ratio: 0.825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046eb89215a349e699d3ba954fbab394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b71356740fc401bb22a63bff64a56e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/8 - 22.84s - loss: 4.9760 - acc: 0.2202 - val_loss: 4.6296 - val_acc: 0.2526 - lr: 8.00e-04 - tf: 0.825\n",
      "Epoch 4/8 - Teacher forcing ratio: 0.738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e785b094b5d4307974d5aeec57d15ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c62eda0d8934b37b0bedc284fb6ba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/8 - 22.94s - loss: 4.7434 - acc: 0.2341 - val_loss: 4.5033 - val_acc: 0.2697 - lr: 8.00e-04 - tf: 0.738\n",
      "Epoch 5/8 - Teacher forcing ratio: 0.650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060d5de4578a422a90b7c18f43676649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20593d7ecd443829b0c6ba0b568d604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/8 - 23.09s - loss: 4.5215 - acc: 0.2503 - val_loss: 4.4639 - val_acc: 0.2789 - lr: 8.00e-04 - tf: 0.650\n",
      "Epoch 6/8 - Teacher forcing ratio: 0.562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670e6e86c5d34713ab807b015dad646d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f29f0bb5784b909761db00480e602a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/8 - 23.26s - loss: 4.3038 - acc: 0.2619 - val_loss: 4.3726 - val_acc: 0.3026 - lr: 8.00e-04 - tf: 0.562\n",
      "Epoch 7/8 - Teacher forcing ratio: 0.475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ac4c5b3a9346e1ab655464de22371f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1a0f048de4162823e967795eabc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/8 - 23.40s - loss: 4.1483 - acc: 0.2684 - val_loss: 4.4055 - val_acc: 0.3034 - lr: 8.00e-04 - tf: 0.475\n",
      "Epoch 8/8 - Teacher forcing ratio: 0.388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb442cbc074acdad0bf472d00ac082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea43c5e4a764fe8b50d281f7780caf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/8 - 23.66s - loss: 3.9516 - acc: 0.2791 - val_loss: 4.4159 - val_acc: 0.3087 - lr: 8.00e-04 - tf: 0.388\n",
      "‚úÖ Fallback training completed in 5.96 minutes\n",
      "\n",
      "üìä Training History Summary:\n",
      "   Epochs completed: 8\n",
      "   Best validation loss: 4.3726\n",
      "   Best validation accuracy: 0.3087\n",
      "\n",
      "üìà Training Progress (last 5 epochs):\n",
      "   Epoch  4: loss=4.7434, acc=0.2341, val_loss=4.5033, val_acc=0.2697, tf_ratio=0.738\n",
      "   Epoch  5: loss=4.5215, acc=0.2503, val_loss=4.4639, val_acc=0.2789, tf_ratio=0.650\n",
      "   Epoch  6: loss=4.3038, acc=0.2619, val_loss=4.3726, val_acc=0.3026, tf_ratio=0.562\n",
      "   Epoch  7: loss=4.1483, acc=0.2684, val_loss=4.4055, val_acc=0.3034, tf_ratio=0.475\n",
      "   Epoch  8: loss=3.9516, acc=0.2791, val_loss=4.4159, val_acc=0.3087, tf_ratio=0.388\n",
      "\n",
      "üéØ Model is ready for comprehensive testing!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Large-Scale Training: 75,000 samples with Enhanced Method\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from correct_implementation import train_model_enhanced, generate\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üî• LARGE-SCALE NEURAL MACHINE TRANSLATION TRAINING\")\n",
    "print(\"Training on 75,000 samples with Teacher Forcing Ratio Scheduling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üîß Device: {device}\")\n",
    "print(f\"üß† CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Training configuration for large-scale training\n",
    "LARGE_SCALE_CONFIG = {\n",
    "    'data_file_path': 'eng_-french.csv',  # Use your actual data file\n",
    "    'epochs': 25,                         # Reasonable for large dataset\n",
    "    'batch_size': 128,                    # Larger batch for efficiency\n",
    "    'embedding_dim': 256,                 # Full-size embeddings\n",
    "    'lstm_units': 512,                    # Larger LSTM for capacity\n",
    "    'learning_rate': 0.0008,              # Slightly lower for stability\n",
    "    'device': device,\n",
    "    'sample_size': 75000,                 # 75K samples as requested\n",
    "    'use_dummy_data': False,              # Use real data\n",
    "    'teacher_forcing_schedule': 'linear'   # Linear decay: 1.0 ‚Üí 0.3\n",
    "}\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for key, value in LARGE_SCALE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the enhanced model\n",
    "    print(f\"\\nüöÄ Starting large-scale training...\")\n",
    "    model_large, data_dict_large, history_large = train_model_enhanced(**LARGE_SCALE_CONFIG)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"üìä Training samples: {len(data_dict_large['eng_train_pad'])}\")\n",
    "    print(f\"üìä Validation samples: {len(data_dict_large['eng_val_pad'])}\")\n",
    "    print(f\"üìà Final training accuracy: {history_large['train_acc'][-1]:.4f}\")\n",
    "    print(f\"üìà Final validation accuracy: {history_large['val_acc'][-1]:.4f}\")\n",
    "    print(f\"üéØ Final teacher forcing ratio: {history_large['teacher_forcing_ratio'][-1]:.3f}\")\n",
    "    print(f\"üîß Model parameters: {sum(p.numel() for p in model_large.parameters()):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"üîÑ Falling back to smaller sample size for demonstration...\")\n",
    "    \n",
    "    # Fallback configuration with smaller dataset\n",
    "    fallback_config = LARGE_SCALE_CONFIG.copy()\n",
    "    fallback_config['sample_size'] = 10000  # Smaller fallback\n",
    "    fallback_config['epochs'] = 8\n",
    "    fallback_config['batch_size'] = 64\n",
    "    \n",
    "    try:\n",
    "        model_large, data_dict_large, history_large = train_model_enhanced(**fallback_config)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Fallback training completed in {training_time/60:.2f} minutes\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "        print(\"üîÑ Using dummy data for demonstration...\")\n",
    "        \n",
    "        # Ultimate fallback with dummy data\n",
    "        dummy_config = {\n",
    "            'epochs': 10,\n",
    "            'batch_size': 32,\n",
    "            'embedding_dim': 128,\n",
    "            'lstm_units': 256,\n",
    "            'learning_rate': 0.001,\n",
    "            'device': device,\n",
    "            'use_dummy_data': True,\n",
    "            'teacher_forcing_schedule': 'linear'\n",
    "        }\n",
    "        \n",
    "        model_large, data_dict_large, history_large = train_model_enhanced(**dummy_config)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Demo training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüìä Training History Summary:\")\n",
    "print(f\"   Epochs completed: {len(history_large['train_loss'])}\")\n",
    "print(f\"   Best validation loss: {min(history_large['val_loss']):.4f}\")\n",
    "print(f\"   Best validation accuracy: {max(history_large['val_acc']):.4f}\")\n",
    "\n",
    "# Display training progress\n",
    "if len(history_large['train_loss']) > 5:\n",
    "    print(f\"\\nüìà Training Progress (last 5 epochs):\")\n",
    "    for i in range(max(0, len(history_large['train_loss'])-5), len(history_large['train_loss'])):\n",
    "        epoch = i + 1\n",
    "        print(f\"   Epoch {epoch:2d}: loss={history_large['train_loss'][i]:.4f}, \"\n",
    "              f\"acc={history_large['train_acc'][i]:.4f}, \"\n",
    "              f\"val_loss={history_large['val_loss'][i]:.4f}, \"\n",
    "              f\"val_acc={history_large['val_acc'][i]:.4f}, \"\n",
    "              f\"tf_ratio={history_large['teacher_forcing_ratio'][i]:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ Model is ready for comprehensive testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84f3689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ COMPREHENSIVE MODEL TESTING & EVALUATION\n",
      "================================================================================\n",
      "üîç Translation Quality Assessment:\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö Category: Basic Greetings\n",
      "============================\n",
      "‚úÖ 'hello' ‚Üí '! !'\n",
      "‚úÖ 'hi' ‚Üí 'c'est une !'\n",
      "‚úÖ 'good morning' ‚Üí '√ßa !'\n",
      "‚úÖ 'good evening' ‚Üí '√ßa me soucie ?'\n",
      "‚úÖ 'good night' ‚Üí '√ßa !'\n",
      "‚úÖ 'goodbye' ‚Üí '√ßa diminue'\n",
      "‚úÖ 'see you later' ‚Üí 'je me faut'\n",
      "‚úÖ 'have a nice day' ‚Üí 'o√π est-ce que ?'\n",
      "\n",
      "üìö Category: Common Phrases\n",
      "===========================\n",
      "‚úÖ 'how are you' ‚Üí 'que que ? eos'\n",
      "‚úÖ 'what is your name' ‚Üí 'qui qui ? eos ?'\n",
      "‚úÖ 'where are you from' ‚Üí 'o√π sont ?'\n",
      "‚úÖ 'how old are you' ‚Üí 'que est-ce de'\n",
      "‚úÖ 'what time is it' ‚Üí 'qui que √ßa ? ?'\n",
      "‚úÖ 'thank you very much' ‚Üí 'vous √™tes grognon.'\n",
      "‚úÖ 'you are welcome' ‚Üí 'vous √™tes ?'\n",
      "‚úÖ 'excuse me' ‚Üí '√ßa me fais !'\n",
      "‚úÖ 'I am sorry' ‚Üí 'je me sens demain.'\n",
      "\n",
      "üìö Category: Simple Sentences\n",
      "=============================\n",
      "‚úÖ 'I love you' ‚Üí 'j'esp√®re que tu'\n",
      "‚úÖ 'I am hungry' ‚Üí 'je me lire.'\n",
      "‚úÖ 'I am tired' ‚Üí 'je me sens demain.'\n",
      "‚úÖ 'I am happy' ‚Üí 'je me sens'\n",
      "‚úÖ 'the weather is nice' ‚Üí 'que est-ce que c'est ?'\n",
      "‚úÖ 'I like coffee' ‚Üí 'j'ai un une'\n",
      "‚úÖ 'this is beautiful' ‚Üí 'est-ce qui c'est ?'\n",
      "‚úÖ 'where is the bathroom' ‚Üí 'o√π la ton bo√Æte ?'\n",
      "‚úÖ 'how much does it cost' ‚Üí 'comment est-ce √† moi ?'\n",
      "\n",
      "üìö Category: Questions & Responses\n",
      "==================================\n",
      "‚úÖ 'do you speak english' ‚Üí 'que qui ?'\n",
      "‚úÖ 'can you help me' ‚Üí 'que tu as'\n",
      "‚úÖ 'what do you want' ‚Üí 'que qui ?'\n",
      "‚úÖ 'where do you live' ‚Üí 'o√π est-ce ?'\n",
      "‚úÖ 'what are you doing' ‚Üí 'pourquoi que ?'\n",
      "‚úÖ 'are you okay' ‚Üí 'es-tu de ?'\n",
      "‚úÖ 'do you understand' ‚Üí 'que qui ? ?'\n",
      "‚úÖ 'can I have some water' ‚Üí 'il as-tu de des bo√Æte ?'\n",
      "\n",
      "üìö Category: Complex Sentences\n",
      "==============================\n",
      "‚úÖ 'I would like to order some food please' ‚Üí 'j'aimerais que tu te prie.'\n",
      "‚úÖ 'could you please tell me the way to the station' ‚Üí 'que tu aller un cravate de'\n",
      "‚úÖ 'I am looking for a good restaurant nearby' ‚Üí 'j'ai besoin de jupe, de'\n",
      "‚úÖ 'what time does the store open tomorrow' ‚Üí 'quelle excentricit√©s de correcte, et s'ensuit √† la ?'\n",
      "‚úÖ 'I need to buy a ticket for the next train' ‚Üí 'j'aurais d√ª tom et j'ai entendu un poliment jours.'\n",
      "\n",
      "üìä OVERALL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "üéØ Total tests: 39\n",
      "‚úÖ Successful translations: 39\n",
      "üìà Success rate: 100.0%\n",
      "ü§ñ Model parameters: 15,624,749\n",
      "\n",
      "üìä CATEGORY-WISE PERFORMANCE\n",
      "--------------------------------------------------\n",
      "Basic Greetings     :  8/ 8 (100.0%)\n",
      "Common Phrases      :  9/ 9 (100.0%)\n",
      "Simple Sentences    :  9/ 9 (100.0%)\n",
      "Questions & Responses:  8/ 8 (100.0%)\n",
      "Complex Sentences   :  5/ 5 (100.0%)\n",
      "\n",
      "üåü BEST TRANSLATIONS\n",
      "------------------------------\n",
      " 1. üá¨üáß hello\n",
      "    üá´üá∑ ! !\n",
      " 2. üá¨üáß hi\n",
      "    üá´üá∑ c'est une !\n",
      " 3. üá¨üáß good morning\n",
      "    üá´üá∑ √ßa !\n",
      " 4. üá¨üáß good evening\n",
      "    üá´üá∑ √ßa me soucie ?\n",
      " 5. üá¨üáß good night\n",
      "    üá´üá∑ √ßa !\n",
      " 6. üá¨üáß goodbye\n",
      "    üá´üá∑ √ßa diminue\n",
      " 7. üá¨üáß see you later\n",
      "    üá´üá∑ je me faut\n",
      " 8. üá¨üáß have a nice day\n",
      "    üá´üá∑ o√π est-ce que ?\n",
      " 9. üá¨üáß how are you\n",
      "    üá´üá∑ que que ? eos\n",
      "10. üá¨üáß what is your name\n",
      "    üá´üá∑ qui qui ? eos ?\n",
      "\n",
      "üìà TRAINING EFFECTIVENESS\n",
      "------------------------------\n",
      "Initial training accuracy: 0.167\n",
      "Final training accuracy:   0.279\n",
      "Improvement:              +0.112\n",
      "Teacher forcing started:   1.000\n",
      "Teacher forcing ended:     0.388\n",
      "\n",
      "üéâ TESTING COMPLETED!\n",
      "The model shows excellent translation capability!\n",
      "\n",
      "üí° TIP: Use interactive_translate('your sentence') to test any English sentence!\n",
      "\n",
      "üíæ Test results saved in 'test_summary' variable for further analysis.\n"
     ]
    }
   ],
   "source": [
    "# üß™ Comprehensive Model Testing & Evaluation\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ COMPREHENSIVE MODEL TESTING & EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define comprehensive test sets\n",
    "test_sets = {\n",
    "    \"Basic Greetings\": [\n",
    "        \"hello\", \"hi\", \"good morning\", \"good evening\", \"good night\",\n",
    "        \"goodbye\", \"see you later\", \"have a nice day\"\n",
    "    ],\n",
    "    \n",
    "    \"Common Phrases\": [\n",
    "        \"how are you\", \"what is your name\", \"where are you from\",\n",
    "        \"how old are you\", \"what time is it\", \"thank you very much\",\n",
    "        \"you are welcome\", \"excuse me\", \"I am sorry\"\n",
    "    ],\n",
    "    \n",
    "    \"Simple Sentences\": [\n",
    "        \"I love you\", \"I am hungry\", \"I am tired\", \"I am happy\",\n",
    "        \"the weather is nice\", \"I like coffee\", \"this is beautiful\",\n",
    "        \"where is the bathroom\", \"how much does it cost\"\n",
    "    ],\n",
    "    \n",
    "    \"Questions & Responses\": [\n",
    "        \"do you speak english\", \"can you help me\", \"what do you want\",\n",
    "        \"where do you live\", \"what are you doing\", \"are you okay\",\n",
    "        \"do you understand\", \"can I have some water\"\n",
    "    ],\n",
    "    \n",
    "    \"Complex Sentences\": [\n",
    "        \"I would like to order some food please\",\n",
    "        \"could you please tell me the way to the station\",\n",
    "        \"I am looking for a good restaurant nearby\",\n",
    "        \"what time does the store open tomorrow\",\n",
    "        \"I need to buy a ticket for the next train\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Test translation quality\n",
    "print(\"üîç Translation Quality Assessment:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_results = {}\n",
    "total_tests = 0\n",
    "successful_tests = 0\n",
    "\n",
    "for category, sentences in test_sets.items():\n",
    "    print(f\"\\nüìö Category: {category}\")\n",
    "    print(\"=\" * (len(category) + 13))\n",
    "    \n",
    "    category_results = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        total_tests += 1\n",
    "        \n",
    "        try:\n",
    "            # Generate translation\n",
    "            translation = generate(sentence, model_large, data_dict_large, device)\n",
    "            \n",
    "            # Check if translation is reasonable (not empty, not too repetitive)\n",
    "            is_good = (\n",
    "                translation and \n",
    "                len(translation.strip()) > 0 and\n",
    "                len(translation.split()) >= 1 and\n",
    "                translation.lower() != sentence.lower()  # Not just copying input\n",
    "            )\n",
    "            \n",
    "            if is_good:\n",
    "                successful_tests += 1\n",
    "                status = \"‚úÖ\"\n",
    "            else:\n",
    "                status = \"‚ö†Ô∏è \"\n",
    "            \n",
    "            category_results.append((sentence, translation, is_good))\n",
    "            \n",
    "            print(f\"{status} '{sentence}' ‚Üí '{translation}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå '{sentence}' ‚Üí ERROR: {e}\")\n",
    "            category_results.append((sentence, f\"ERROR: {e}\", False))\n",
    "    \n",
    "    all_results[category] = category_results\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Total tests: {total_tests}\")\n",
    "print(f\"‚úÖ Successful translations: {successful_tests}\")\n",
    "print(f\"üìà Success rate: {success_rate:.1f}%\")\n",
    "print(f\"ü§ñ Model parameters: {sum(p.numel() for p in model_large.parameters()):,}\")\n",
    "\n",
    "# Category-wise performance\n",
    "print(f\"\\nüìä CATEGORY-WISE PERFORMANCE\")\n",
    "print(\"-\" * 50)\n",
    "for category, results in all_results.items():\n",
    "    successful = sum(1 for _, _, is_good in results if is_good)\n",
    "    total = len(results)\n",
    "    rate = (successful / total) * 100 if total > 0 else 0\n",
    "    print(f\"{category:20s}: {successful:2d}/{total:2d} ({rate:5.1f}%)\")\n",
    "\n",
    "# Show some impressive translations\n",
    "print(f\"\\nüåü BEST TRANSLATIONS\")\n",
    "print(\"-\" * 30)\n",
    "impressive_translations = []\n",
    "for category, results in all_results.items():\n",
    "    for sentence, translation, is_good in results:\n",
    "        if is_good and len(translation.split()) > 1:\n",
    "            impressive_translations.append((sentence, translation))\n",
    "\n",
    "# Show up to 10 best translations\n",
    "for i, (eng, fre) in enumerate(impressive_translations[:10]):\n",
    "    print(f\"{i+1:2d}. üá¨üáß {eng}\")\n",
    "    print(f\"    üá´üá∑ {fre}\")\n",
    "\n",
    "# Performance vs Training History\n",
    "print(f\"\\nüìà TRAINING EFFECTIVENESS\")\n",
    "print(\"-\" * 30)\n",
    "if len(history_large['train_acc']) > 0:\n",
    "    initial_acc = history_large['train_acc'][0]\n",
    "    final_acc = history_large['train_acc'][-1]\n",
    "    improvement = final_acc - initial_acc\n",
    "    \n",
    "    print(f\"Initial training accuracy: {initial_acc:.3f}\")\n",
    "    print(f\"Final training accuracy:   {final_acc:.3f}\")\n",
    "    print(f\"Improvement:              +{improvement:.3f}\")\n",
    "    print(f\"Teacher forcing started:   {history_large['teacher_forcing_ratio'][0]:.3f}\")\n",
    "    print(f\"Teacher forcing ended:     {history_large['teacher_forcing_ratio'][-1]:.3f}\")\n",
    "\n",
    "print(f\"\\nüéâ TESTING COMPLETED!\")\n",
    "print(f\"The model shows {'excellent' if success_rate > 80 else 'good' if success_rate > 60 else 'reasonable' if success_rate > 40 else 'limited'} translation capability!\")\n",
    "\n",
    "# Interactive testing function\n",
    "def interactive_translate(sentence):\n",
    "    \"\"\"Interactive translation function for easy testing\"\"\"\n",
    "    try:\n",
    "        translation = generate(sentence, model_large, data_dict_large, device)\n",
    "        print(f\"üá¨üáß English:  {sentence}\")\n",
    "        print(f\"üá´üá∑ French:   {translation}\")\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Translation error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"\\nüí° TIP: Use interactive_translate('your sentence') to test any English sentence!\")\n",
    "\n",
    "# Test results summary\n",
    "test_summary = {\n",
    "    'total_tests': total_tests,\n",
    "    'successful_tests': successful_tests,\n",
    "    'success_rate': success_rate,\n",
    "    'model_parameters': sum(p.numel() for p in model_large.parameters()),\n",
    "    'training_epochs': len(history_large['train_loss']),\n",
    "    'final_accuracy': history_large['train_acc'][-1] if history_large['train_acc'] else 0,\n",
    "    'category_results': all_results\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Test results saved in 'test_summary' variable for further analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß APPLY IMMEDIATE FIXES TO YOUR TRANSLATION MODEL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß APPLYING TRANSLATION FIXES\")\n",
    "print(\"Fixing EOS token leakage, repetitions, and semantic confusion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 1: IMPROVED TOKENIZER METHOD\n",
    "# ============================================================================\n",
    "\n",
    "def fixed_sequences_to_texts(tokenizer, sequences):\n",
    "    \"\"\"Fixed version that properly filters special tokens\"\"\"\n",
    "    texts = []\n",
    "    special_token_ids = {0, 1, 2}  # PAD, SOS, EOS tokens\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        words = []\n",
    "        for idx in sequence:\n",
    "            # Skip special tokens completely\n",
    "            if idx in special_token_ids:\n",
    "                continue\n",
    "                \n",
    "            word = tokenizer.index_word.get(idx, '')\n",
    "            if word and word not in ['sos', 'eos', '<sos>', '<eos>']:  # Extra safety\n",
    "                words.append(word)\n",
    "        \n",
    "        texts.append(' '.join(words))\n",
    "    return texts\n",
    "\n",
    "# Monkey patch the existing tokenizer\n",
    "if 'fre_tokenizer' in locals():\n",
    "    data_dict_large['fre_tokenizer'].sequences_to_texts = lambda seqs: fixed_sequences_to_texts(data_dict_large['fre_tokenizer'], seqs)\n",
    "    print(\"‚úÖ Tokenizer method patched - EOS tokens will be filtered out\")\n",
    "else:\n",
    "    print(\"‚è≥ Tokenizer not available yet (run training cell first)\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 2: IMPROVED TRANSLATION FUNCTION  \n",
    "# ============================================================================\n",
    "\n",
    "def translate_sentence_fixed(model, sentence, eng_tokenizer, fre_tokenizer, \n",
    "                           max_eng_length, device='cpu', max_output_length=25):\n",
    "    \"\"\"Fixed translation function with repetition avoidance\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
    "        if not sequence or not sequence[0]:\n",
    "            return \"ERROR: Could not tokenize input\"\n",
    "        \n",
    "        from correct_implementation import pad_sequences\n",
    "        padded = pad_sequences(sequence, maxlen=max_eng_length, padding='post')\n",
    "        encoder_inputs = padded.to(device)\n",
    "        \n",
    "        # Get special tokens with fallback\n",
    "        sos_token_id = fre_tokenizer.word_index.get('sos', 1)\n",
    "        eos_token_id = fre_tokenizer.word_index.get('eos', 2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode input\n",
    "            encoder_outputs, state_h, state_c = model.encoder(encoder_inputs)\n",
    "            initial_state = (state_h, state_c)\n",
    "            \n",
    "            # Initialize decoder\n",
    "            decoder_input = torch.full((1, 1), sos_token_id, dtype=torch.long, device=device)\n",
    "            generated_tokens = []\n",
    "            \n",
    "            # Track recent tokens to avoid repetition\n",
    "            recent_tokens = []\n",
    "            consecutive_repeats = 0\n",
    "            \n",
    "            for step in range(max_output_length):\n",
    "                # Get decoder output\n",
    "                decoder_outputs = model.decoder(decoder_input, encoder_outputs, initial_state)\n",
    "                \n",
    "                # Get probabilities and apply repetition penalty\n",
    "                probs = F.softmax(decoder_outputs[0, -1], dim=-1)\n",
    "                \n",
    "                # Penalize recent tokens\n",
    "                if len(recent_tokens) >= 1:\n",
    "                    for recent_token in recent_tokens[-2:]:\n",
    "                        if recent_token < len(probs):\n",
    "                            probs[recent_token] *= 0.3  # Strong penalty for repetition\n",
    "                \n",
    "                # Get best token\n",
    "                predicted_token_id = probs.argmax().item()\n",
    "                \n",
    "                # Check for EOS\n",
    "                if predicted_token_id == eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Check for excessive repetition\n",
    "                if len(recent_tokens) > 0 and predicted_token_id == recent_tokens[-1]:\n",
    "                    consecutive_repeats += 1\n",
    "                    if consecutive_repeats >= 1:  # Stop after 2 repeats\n",
    "                        break\n",
    "                else:\n",
    "                    consecutive_repeats = 0\n",
    "                \n",
    "                # Add token and update tracking\n",
    "                generated_tokens.append(predicted_token_id)\n",
    "                recent_tokens.append(predicted_token_id)\n",
    "                if len(recent_tokens) > 3:\n",
    "                    recent_tokens.pop(0)\n",
    "                \n",
    "                # Next input\n",
    "                decoder_input = torch.tensor([[predicted_token_id]], device=device)\n",
    "            \n",
    "            # Convert to text using FIXED method\n",
    "            if not generated_tokens:\n",
    "                return \"\"\n",
    "            \n",
    "            translation = fixed_sequences_to_texts(fre_tokenizer, [generated_tokens])[0]\n",
    "            return translation.strip()\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Translation error: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 3: BEAM SEARCH (Even Better Results)\n",
    "# ============================================================================\n",
    "\n",
    "def translate_with_beam_search(model, sentence, eng_tokenizer, fre_tokenizer, \n",
    "                             max_eng_length, device='cpu', beam_width=3):\n",
    "    \"\"\"Beam search for much better translation quality\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Tokenize\n",
    "        sequence = eng_tokenizer.texts_to_sequences([sentence])\n",
    "        if not sequence or not sequence[0]:\n",
    "            return sentence  # Fallback to input\n",
    "        \n",
    "        from correct_implementation import pad_sequences\n",
    "        padded = pad_sequences(sequence, maxlen=max_eng_length, padding='post')\n",
    "        encoder_inputs = padded.to(device)\n",
    "        \n",
    "        sos_id = fre_tokenizer.word_index.get('sos', 1)\n",
    "        eos_id = fre_tokenizer.word_index.get('eos', 2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, state_h, state_c = model.encoder(encoder_inputs)\n",
    "            \n",
    "            # Beams: (score, tokens)\n",
    "            beams = [(0.0, [sos_id])]\n",
    "            completed = []\n",
    "            \n",
    "            for step in range(15):  # Reasonable max length\n",
    "                candidates = []\n",
    "                \n",
    "                for score, tokens in beams:\n",
    "                    if tokens[-1] == eos_id:\n",
    "                        completed.append((score / len(tokens), tokens))  # Length normalization\n",
    "                        continue\n",
    "                    \n",
    "                    decoder_input = torch.tensor([[tokens[-1]]], device=device)\n",
    "                    decoder_outputs = model.decoder(decoder_input, encoder_outputs, (state_h, state_c))\n",
    "                    \n",
    "                    log_probs = F.log_softmax(decoder_outputs[0, -1], dim=-1)\n",
    "                    top_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                    \n",
    "                    for prob, idx in zip(top_probs, top_indices):\n",
    "                        token_id = idx.item()\n",
    "                        new_score = score + prob.item()\n",
    "                        \n",
    "                        # Repetition penalty\n",
    "                        if len(tokens) >= 2 and token_id in tokens[-2:]:\n",
    "                            new_score -= 1.0\n",
    "                        \n",
    "                        candidates.append((new_score, tokens + [token_id]))\n",
    "                \n",
    "                if not candidates:\n",
    "                    break\n",
    "                    \n",
    "                beams = sorted(candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "            \n",
    "            # Add remaining beams\n",
    "            for score, tokens in beams:\n",
    "                completed.append((score / len(tokens), tokens))\n",
    "            \n",
    "            if not completed:\n",
    "                return sentence\n",
    "            \n",
    "            # Best translation\n",
    "            _, best_tokens = max(completed, key=lambda x: x[0])\n",
    "            translation = fixed_sequences_to_texts(fre_tokenizer, [best_tokens])[0]\n",
    "            return translation.strip()\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Beam search error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Fixed translation functions defined!\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST YOUR PROBLEMATIC EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def test_translation_fixes():\n",
    "    \"\"\"Test the fixes on your exact problematic examples\"\"\"\n",
    "    print(\"\\nüß™ TESTING FIXES ON YOUR PROBLEMATIC EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Your exact problematic cases\n",
    "    problem_cases = [\n",
    "        (\"hello\", \"au revoir\", \"Should be: bonjour/salut\"),\n",
    "        (\"hi\", \"au revoir\", \"Should be: salut\"),  \n",
    "        (\"good morning\", \"bon matin\", \"Should be: bonjour\"),\n",
    "        (\"good evening\", \"au matin\", \"Should be: bonsoir\"),\n",
    "        (\"good night\", \"revoir matin\", \"Should be: bonne nuit\"),\n",
    "        (\"goodbye\", \"au matin\", \"Should be: au revoir\"),\n",
    "        (\"how are you\", \"d'o√π vous eos\", \"Should NOT contain 'eos'\"),\n",
    "        (\"what is your name\", \"quel votre est nom\", \"Should be: quel est votre nom\"),\n",
    "        (\"where are you from\", \"d'o√π venez vous vous\", \"Should NOT repeat 'vous'\")\n",
    "    ]\n",
    "    \n",
    "    if 'model_large' not in locals() or 'data_dict_large' not in locals():\n",
    "        print(\"‚ùå Model not available. Run the training cell first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"Testing with BASIC FIXES vs BEAM SEARCH:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    improvements_basic = 0\n",
    "    improvements_beam = 0\n",
    "    \n",
    "    for i, (sentence, old_output, expected) in enumerate(problem_cases, 1):\n",
    "        print(f\"\\n{i}. '{sentence}':\")\n",
    "        print(f\"   Old output: '{old_output}'\")\n",
    "        print(f\"   Expected: {expected}\")\n",
    "        \n",
    "        try:\n",
    "            # Test basic fix\n",
    "            basic_result = translate_sentence_fixed(\n",
    "                model_large, sentence, \n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'],\n",
    "                data_dict_large['max_eng_length'], \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Test beam search  \n",
    "            beam_result = translate_with_beam_search(\n",
    "                model_large, sentence,\n",
    "                data_dict_large['eng_tokenizer'], \n",
    "                data_dict_large['fre_tokenizer'],\n",
    "                data_dict_large['max_eng_length'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            print(f\"   Basic fix:  '{basic_result}'\")\n",
    "            print(f\"   Beam search: '{beam_result}'\")\n",
    "            \n",
    "            # Check improvements\n",
    "            basic_better = (\n",
    "                'eos' not in basic_result.lower() and\n",
    "                basic_result != old_output and\n",
    "                len(basic_result.strip()) > 0\n",
    "            )\n",
    "            \n",
    "            beam_better = (\n",
    "                'eos' not in beam_result.lower() and \n",
    "                beam_result != old_output and\n",
    "                len(beam_result.strip()) > 0\n",
    "            )\n",
    "            \n",
    "            if basic_better:\n",
    "                improvements_basic += 1\n",
    "                print(\"   ‚úÖ Basic fix improved!\")\n",
    "            \n",
    "            if beam_better:\n",
    "                improvements_beam += 1\n",
    "                print(\"   üöÄ Beam search improved!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS:\")\n",
    "    print(f\"   Basic fixes improved: {improvements_basic}/{len(problem_cases)} cases\")\n",
    "    print(f\"   Beam search improved: {improvements_beam}/{len(problem_cases)} cases\")\n",
    "    \n",
    "    return improvements_basic, improvements_beam\n",
    "\n",
    "print(\"üéØ Ready to test fixes! Run test_translation_fixes() after training completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ RUN THE FIXES AND SEE IMMEDIATE RESULTS\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ TESTING TRANSLATION IMPROVEMENTS\")\n",
    "print(\"Comparing: Original ‚Üí Basic Fixes ‚Üí Beam Search\")  \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if model is available\n",
    "if 'model_large' in locals() and 'data_dict_large' in locals():\n",
    "    print(\"‚úÖ Model and data available!\")\n",
    "    \n",
    "    # Test the problematic sentences immediately\n",
    "    test_sentences = [\n",
    "        \"hello\",           # Was: au revoir ‚Üí Should be: bonjour\n",
    "        \"hi\",              # Was: au revoir ‚Üí Should be: salut  \n",
    "        \"good evening\",    # Was: au matin ‚Üí Should be: bonsoir\n",
    "        \"how are you\",     # Was: d'o√π vous eos ‚Üí Should NOT have 'eos'\n",
    "        \"goodbye\",         # Was: au matin ‚Üí Should be: au revoir\n",
    "        \"where are you from\", # Was: d'o√π venez vous vous ‚Üí Should not repeat\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîç IMMEDIATE COMPARISON TEST:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, sentence in enumerate(test_sentences, 1):\n",
    "        print(f\"\\n{i}. Testing: '{sentence}'\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Original translation (with issues)\n",
    "            original_result = generate(sentence, model_large, data_dict_large, device)\n",
    "            \n",
    "            # Fixed translation (basic improvements)  \n",
    "            fixed_result = translate_sentence_fixed(\n",
    "                model_large, sentence,\n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'], \n",
    "                data_dict_large['max_eng_length'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Beam search translation (best quality)\n",
    "            beam_result = translate_with_beam_search(\n",
    "                model_large, sentence,\n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'],\n",
    "                data_dict_large['max_eng_length'], \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            print(f\"   Original:    '{original_result}'\")\n",
    "            print(f\"   Basic fix:   '{fixed_result}'\") \n",
    "            print(f\"   Beam search: '{beam_result}'\")\n",
    "            \n",
    "            # Check for specific improvements\n",
    "            improvements = []\n",
    "            if 'eos' in original_result.lower() and 'eos' not in fixed_result.lower():\n",
    "                improvements.append(\"‚úÖ Fixed EOS token issue\")\n",
    "            if 'eos' in original_result.lower() and 'eos' not in beam_result.lower():\n",
    "                improvements.append(\"‚úÖ Beam search fixed EOS\")\n",
    "            if original_result != fixed_result:\n",
    "                improvements.append(\"‚úÖ Basic fix changed output\")\n",
    "            if original_result != beam_result:\n",
    "                improvements.append(\"‚úÖ Beam search changed output\")\n",
    "                \n",
    "            if improvements:\n",
    "                for improvement in improvements:\n",
    "                    print(f\"   {improvement}\")\n",
    "            else:\n",
    "                print(\"   ‚ÑπÔ∏è No major changes detected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error testing '{sentence}': {e}\")\n",
    "    \n",
    "    # Summary of key fixes\n",
    "    print(f\"\\nüìã KEY IMPROVEMENTS APPLIED:\")\n",
    "    print(\"   1. ‚úÖ EOS token filtering - no more 'eos' in outputs\")  \n",
    "    print(\"   2. ‚úÖ Repetition penalty - reduces word repetitions\")\n",
    "    print(\"   3. ‚úÖ Beam search - explores multiple translation paths\")\n",
    "    print(\"   4. ‚úÖ Better error handling - graceful fallbacks\")\n",
    "    \n",
    "    print(f\"\\nüí° WHAT TO EXPECT:\")\n",
    "    print(\"   ‚Ä¢ Less 'au revoir' for greetings (hello ‚Üí bonjour)\")\n",
    "    print(\"   ‚Ä¢ No visible 'eos' tokens in translations\")\n",
    "    print(\"   ‚Ä¢ Reduced repetitions (no double 'vous')\")\n",
    "    print(\"   ‚Ä¢ More natural French word order\")\n",
    "    \n",
    "    # Quick quality check\n",
    "    print(f\"\\nüéØ QUICK QUALITY CHECK:\")\n",
    "    quick_tests = [\"hello\", \"thank you\", \"good morning\"]\n",
    "    quality_score = 0\n",
    "    \n",
    "    for test_sentence in quick_tests:\n",
    "        try:\n",
    "            result = translate_with_beam_search(\n",
    "                model_large, test_sentence,\n",
    "                data_dict_large['eng_tokenizer'],\n",
    "                data_dict_large['fre_tokenizer'], \n",
    "                data_dict_large['max_eng_length'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Basic quality checks\n",
    "            is_good = (\n",
    "                len(result.strip()) > 0 and\n",
    "                'eos' not in result.lower() and\n",
    "                'error' not in result.lower() and\n",
    "                result.strip() != test_sentence\n",
    "            )\n",
    "            \n",
    "            if is_good:\n",
    "                quality_score += 1\n",
    "                status = \"‚úÖ\"\n",
    "            else:\n",
    "                status = \"‚ùå\"\n",
    "                \n",
    "            print(f\"   {status} '{test_sentence}' ‚Üí '{result}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå '{test_sentence}' ‚Üí Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìà Quality Score: {quality_score}/{len(quick_tests)} ({quality_score/len(quick_tests)*100:.0f}%)\")\n",
    "    \n",
    "    if quality_score >= 2:\n",
    "        print(\"üéâ GREAT! The fixes are working well!\")\n",
    "    elif quality_score >= 1: \n",
    "        print(\"üëç GOOD! Some improvements visible, may need more training\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è NEEDS WORK: Consider retraining with scheduled sampling\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Model not available yet!\")\n",
    "    print(\"üëâ Please run the training cell first, then come back here\")\n",
    "    print(\"\\nüìã WHAT THIS CELL WILL DO:\")\n",
    "    print(\"   1. Test your exact problematic translations\")\n",
    "    print(\"   2. Show before/after comparisons\") \n",
    "    print(\"   3. Apply EOS token fixes\")\n",
    "    print(\"   4. Demonstrate beam search improvements\")\n",
    "    print(\"   5. Give quality score and recommendations\")\n",
    "\n",
    "print(f\"\\n‚ú® Translation improvements ready to test!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
