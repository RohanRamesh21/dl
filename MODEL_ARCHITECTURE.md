# Neural Machine Translation - Model Architecture Flow

## ğŸ—ï¸ **Complete System Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          NEURAL MACHINE TRANSLATION SYSTEM                   â”‚
â”‚                            (English â†’ French)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: "hello world"
   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ“¥ DATA PREPROCESSING PIPELINE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. TOKENIZATION
   â”œâ”€ Input: "hello world"
   â”œâ”€ Tokenizer.texts_to_sequences()
   â””â”€ Output: [45, 231]  (word indices)

2. PADDING
   â”œâ”€ Input: [45, 231]
   â”œâ”€ pad_sequences(maxlen=20)
   â””â”€ Output: [45, 231, 0, 0, 0, ..., 0]  (shape: [20])

3. TENSOR CONVERSION
   â””â”€ PyTorch Tensor: shape [1, 20]  (batch_size=1, seq_len=20)

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ§  ENCODER (BIDIRECTIONAL LSTM)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT TENSOR: [batch_size, src_seq_len] = [1, 20]
   â†“
   
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EMBEDDING LAYER        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  Input: [1, 20]         â”‚
â”‚  Lookup in embedding    â”‚
â”‚  matrix [vocab, 256]    â”‚
â”‚  Output: [1, 20, 256]   â”‚  â† Each word â†’ 256-dim vector
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MULTI-LAYER BIDIRECTIONAL LSTM                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚                                                                  â”‚
â”‚  Layer 1: Bidirectional LSTM                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  FORWARD â†’   â”‚         â”‚  â† BACKWARD  â”‚                     â”‚
â”‚  â”‚  LSTM Cell   â”‚         â”‚  LSTM Cell   â”‚                     â”‚
â”‚  â”‚  [1,20,256]  â”‚         â”‚  [1,20,256]  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â†“                         â†“                             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONCAT â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                 â†“                                                â”‚
â”‚         [1, 20, 512]  (forward + backward)                      â”‚
â”‚                 â†“                                                â”‚
â”‚            DROPOUT (0.1)                                         â”‚
â”‚                 â†“                                                â”‚
â”‚  Layer 2: Bidirectional LSTM (if num_layers > 1)               â”‚
â”‚         [1, 20, 512] â†’ ... â†’ [1, 20, 512]                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“

ENCODER OUTPUTS:
â”œâ”€ encoder_outputs: [1, 20, 512]  â† All hidden states (for attention)
â”œâ”€ state_h: [1, 256]              â† Final forward hidden state
â””â”€ state_c: [1, 256]              â† Final forward cell state

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸ¯ DECODER (UNIDIRECTIONAL LSTM + ATTENTION)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INITIALIZATION:
â”œâ”€ decoder_input: [1, 1] = [SOS_TOKEN_ID]  â† Start token
â””â”€ decoder_state: (state_h, state_c) from encoder

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STEP-BY-STEP GENERATION LOOP                   â”‚
â”‚                  (Repeat for each output word)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For t = 0, 1, 2, ..., max_length:

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  EMBEDDING LAYER        â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
   â”‚  Input: [1, 1]          â”‚  â† Current decoder input token
   â”‚  Output: [1, 1, 256]    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  MULTI-LAYER LSTM                       â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
   â”‚  Input: [1, 1, 256]                     â”‚
   â”‚  State: (h_t, c_t)                      â”‚
   â”‚  Output: [1, 1, 256]                    â”‚
   â”‚  New State: (h_{t+1}, c_{t+1})          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   lstm_output: [1, 1, 256]
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ATTENTION MECHANISM (Bahdanau/Additive)                     â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
   â”‚                                                               â”‚
   â”‚  Query (from decoder):  [1, 1, 256]                          â”‚
   â”‚  Keys (from encoder):   [1, 20, 512]                         â”‚
   â”‚  Values (from encoder): [1, 20, 512]                         â”‚
   â”‚                                                               â”‚
   â”‚  Step 1: SCORE COMPUTATION                                   â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
   â”‚  query_projection = W_q @ query      â†’ [1, 1, attention_dim]â”‚
   â”‚  key_projection = W_k @ keys         â†’ [1, 20, attention_dim]â”‚
   â”‚  combined = tanh(query_proj + key_proj)                      â”‚
   â”‚  scores = V^T @ combined             â†’ [1, 20]               â”‚
   â”‚                                                               â”‚
   â”‚  Step 2: ATTENTION WEIGHTS (Softmax)                         â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
   â”‚  attention_weights = softmax(scores) â†’ [1, 20]               â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
   â”‚  â”‚ [0.05, 0.32, 0.18, 0.08, 0.01, ..., 0.00]     â”‚          â”‚
   â”‚  â”‚   â†‘     â†‘                                       â”‚          â”‚
   â”‚  â”‚  word1 word2 â† Focuses here!                   â”‚          â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
   â”‚                                                               â”‚
   â”‚  Step 3: CONTEXT VECTOR (Weighted Sum)                       â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
   â”‚  context = Î£(attention_weights * values) â†’ [1, 1, 512]      â”‚
   â”‚                                                               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
   context_vector: [1, 1, 512]
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  CONCATENATION                          â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
   â”‚  [context, lstm_output]                 â”‚
   â”‚  [1, 1, 512] + [1, 1, 256]             â”‚
   â”‚  = [1, 1, 768]                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  OUTPUT PROJECTION (Linear Layer)       â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
   â”‚  Input: [1, 768]                        â”‚
   â”‚  Weight: [768, french_vocab_size]       â”‚
   â”‚  Output: [1, french_vocab_size]         â”‚
   â”‚  = [1, 10000] (logits)                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  SOFTMAX (Probability Distribution)     â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
   â”‚  P(word) for all French vocab           â”‚
   â”‚  [0.001, 0.003, 0.856, ..., 0.002]     â”‚
   â”‚                    â†‘                     â”‚
   â”‚                 highest probability      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  TOKEN SELECTION                        â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
   â”‚  â€¢ Greedy: argmax(logits)               â”‚
   â”‚  â€¢ Sampling: sample with temperature    â”‚
   â”‚                                          â”‚
   â”‚  Selected token ID: 7834 â†’ "bonjour"   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
      
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  NEXT INPUT DECISION                    â”‚
   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
   â”‚  â€¢ Training (Teacher Forcing):          â”‚
   â”‚    Use ground truth next token          â”‚
   â”‚                                          â”‚
   â”‚  â€¢ Inference:                            â”‚
   â”‚    Use predicted token (7834)           â”‚
   â”‚                                          â”‚
   â”‚  â€¢ Teacher Forcing Ratio:               â”‚
   â”‚    Mix of both with probability         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
      
   decoder_input = selected_token
   
   Stop if:
   â”œâ”€ Token == EOS_TOKEN_ID  (End of sequence)
   â”œâ”€ Length >= max_length   (Maximum length reached)
   â””â”€ All sequences in batch finished

LOOP BACK TO TOP â†‘

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ“¤ OUTPUT GENERATION                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GENERATED TOKEN IDs: [1, 7834, 4521, 2]
                     [SOS, "bonjour", "monde", EOS]
   â†“

DETOKENIZATION:
   â”œâ”€ Remove SOS and EOS tokens
   â”œâ”€ Map IDs back to words
   â””â”€ Join words with spaces

OUTPUT: "bonjour monde"

```

---

## ğŸ”„ **Training vs Inference Flow**

### **TRAINING MODE** (with Teacher Forcing)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRAINING STEP                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Batch:
â”œâ”€ encoder_inputs: [batch_size, src_seq_len]
â”œâ”€ decoder_inputs: [batch_size, tgt_seq_len]  â† Ground truth (shifted)
â””â”€ targets: [batch_size, tgt_seq_len]         â† Ground truth

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENCODER           â”‚
â”‚  (Bidirectional)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
encoder_outputs, state_h, state_c

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECODER (Step-by-Step)                    â”‚
â”‚                                             â”‚
â”‚  For each time step t:                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚  Current Input Decision:        â”‚    â”‚
â”‚    â”‚                                  â”‚    â”‚
â”‚    â”‚  if random() < teacher_forcing: â”‚    â”‚
â”‚    â”‚    use decoder_inputs[:, t]     â”‚    â”‚  â† Ground truth
â”‚    â”‚  else:                           â”‚    â”‚
â”‚    â”‚    use predicted_token          â”‚    â”‚  â† Model prediction
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                             â”‚
â”‚  â†’ Embedding â†’ LSTM â†’ Attention â†’          â”‚
â”‚    Concatenate â†’ Output Projection         â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
predictions: [batch_size, tgt_seq_len, vocab_size]

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOSS COMPUTATION                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  loss = CrossEntropy(predictions, targets) â”‚
â”‚  mask out padding tokens (token_id = 0)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
   
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BACKPROPAGATION                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚  loss.backward()                           â”‚
â”‚  â†“                                          â”‚
â”‚  Compute gradients for all parameters      â”‚
â”‚  â†“                                          â”‚
â”‚  Gradient Clipping (prevent exploding)     â”‚
â”‚  â†“                                          â”‚
â”‚  optimizer.step()  â† Update weights        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **INFERENCE MODE** (Generation)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INFERENCE/TRANSLATION                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input: "hello world"

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENCODER           â”‚
â”‚  (Bidirectional)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
encoder_outputs, state_h, state_c

   â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECODER (Autoregressive Generation)       â”‚
â”‚                                             â”‚
â”‚  Initialize:                                â”‚
â”‚  â”œâ”€ decoder_input = [SOS]                  â”‚
â”‚  â””â”€ decoder_state = (state_h, state_c)     â”‚
â”‚                                             â”‚
â”‚  Loop until EOS or max_length:             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚  1. Embed current token         â”‚    â”‚
â”‚    â”‚  2. Pass through LSTM           â”‚    â”‚
â”‚    â”‚  3. Apply attention             â”‚    â”‚
â”‚    â”‚  4. Concatenate & project       â”‚    â”‚
â”‚    â”‚  5. Sample/Select next token    â”‚    â”‚
â”‚    â”‚  6. Append to output sequence   â”‚    â”‚
â”‚    â”‚  7. Use as next input           â”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                             â”‚
â”‚  NO teacher forcing - always use previous  â”‚
â”‚  predicted token as next input             â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
   
Generated Sequence: [SOS, token1, token2, ..., EOS]

   â†“

Detokenize â†’ "bonjour monde"
```

---

## ğŸ“Š **Detailed Component Dimensions**

### **Configuration Example**
```python
batch_size = 64
src_vocab_size = 8000      # English vocabulary
tgt_vocab_size = 10000     # French vocabulary
embedding_dim = 256
lstm_units = 256
encoder_num_layers = 2
decoder_num_layers = 2
dropout_rate = 0.1
bidirectional = True
max_src_length = 20
max_tgt_length = 20
```

### **Tensor Shapes Through the Network**

```
ENCODER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:              [64, 20]           # batch_size Ã— src_seq_len
  â†“ Embedding
Embedded:           [64, 20, 256]      # batch_size Ã— src_seq_len Ã— embedding_dim
  â†“ Bidirectional LSTM Layer 1
Forward Hidden:     [64, 20, 256]
Backward Hidden:    [64, 20, 256]
Concatenated:       [64, 20, 512]      # doubled due to bidirectional
  â†“ Dropout
  â†“ Bidirectional LSTM Layer 2
Encoder Outputs:    [64, 20, 512]      # batch_size Ã— src_seq_len Ã— (lstm_units*2)
Final state_h:      [64, 256]          # batch_size Ã— lstm_units (forward only)
Final state_c:      [64, 256]          # batch_size Ã— lstm_units (forward only)

DECODER (per time step):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Current Input:      [64, 1]            # batch_size Ã— 1
  â†“ Embedding
Embedded:           [64, 1, 256]       # batch_size Ã— 1 Ã— embedding_dim
  â†“ LSTM Layer 1
LSTM Output:        [64, 1, 256]       # batch_size Ã— 1 Ã— lstm_units
  â†“ LSTM Layer 2
LSTM Output:        [64, 1, 256]       # batch_size Ã— 1 Ã— lstm_units

ATTENTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query (decoder):    [64, 1, 256]       # batch_size Ã— 1 Ã— lstm_units
Keys (encoder):     [64, 20, 512]      # batch_size Ã— src_seq_len Ã— encoder_output_size
Values (encoder):   [64, 20, 512]      # batch_size Ã— src_seq_len Ã— encoder_output_size
  â†“ Compute scores
Attention Weights:  [64, 1, 20]        # batch_size Ã— 1 Ã— src_seq_len
  â†“ Weighted sum
Context Vector:     [64, 1, 512]       # batch_size Ã— 1 Ã— encoder_output_size

OUTPUT:
â”€â”€â”€â”€â”€â”€â”€
Concatenated:       [64, 1, 768]       # [context(512) + lstm_output(256)]
  â†“ Flatten to [64, 768]
  â†“ Linear Projection
Logits:             [64, 10000]        # batch_size Ã— tgt_vocab_size
  â†“ Softmax
Probabilities:      [64, 10000]        # batch_size Ã— tgt_vocab_size
```

---

## ğŸ§® **Mathematical Operations**

### **1. LSTM Cell Forward Pass**
```
Gates Computation (for each time step):

Input Gate:     i_t = Ïƒ(W_iiÂ·x_t + W_hiÂ·h_{t-1} + b_i)
Forget Gate:    f_t = Ïƒ(W_ifÂ·x_t + W_hfÂ·h_{t-1} + b_f)
Cell Gate:      g_t = tanh(W_igÂ·x_t + W_hgÂ·h_{t-1} + b_g)
Output Gate:    o_t = Ïƒ(W_ioÂ·x_t + W_hoÂ·h_{t-1} + b_o)

Cell State:     c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t
Hidden State:   h_t = o_t âŠ™ tanh(c_t)

Where:
  Ïƒ = sigmoid activation
  âŠ™ = element-wise multiplication
  W = weight matrices
  b = bias vectors
```

### **2. Attention Mechanism**
```
Additive (Bahdanau) Attention:

1. Project query and keys:
   query_proj = W_q @ query         # [batch, 1, attn_dim]
   key_proj = W_k @ keys            # [batch, src_len, attn_dim]

2. Compute alignment scores:
   combined = query_proj + key_proj # Broadcasting
   activated = tanh(combined)
   scores = V^T @ activated         # [batch, 1, src_len]

3. Attention weights (probabilities):
   Î± = softmax(scores / âˆšd_k)       # [batch, 1, src_len]
   
4. Context vector (weighted sum):
   context = Î£(Î±_i Â· value_i)       # [batch, 1, value_dim]
```

### **3. Loss Function**
```
Sparse Categorical Cross-Entropy:

For each position (i, j):
  loss_ij = -log(P(y_ij | x, y_{<j}))
  
Where:
  y_ij = true token at position j in sequence i
  P = softmax(logits)

Total Loss:
  L = (1/N) Î£ Î£ loss_ij  (masked for padding)
      
Accuracy:
  acc = (correct_predictions / non_padding_tokens) Ã— 100%
```

---

## ğŸ”€ **Bidirectional Processing**

```
UNIDIRECTIONAL (Standard):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  [w1, w2, w3, w4, w5]
        â†“   â†“   â†“   â†“   â†“
       h1â†’ h2â†’ h3â†’ h4â†’ h5â†’

Each h_t only sees past: w1...w_t


BIDIRECTIONAL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:      [w1, w2, w3, w4, w5]

Forward:    h1â†’ h2â†’ h3â†’ h4â†’ h5â†’
Backward:  â†h1 â†h2 â†h3 â†h4 â†h5

Combined:   [h1â†’;â†h1]  [h2â†’;â†h2]  [h3â†’;â†h3]  [h4â†’;â†h4]  [h5â†’;â†h5]
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                 Concatenate
                 
Each output sees both past AND future context!
â†’ Better understanding of sentence structure
â†’ Improved translation quality
```

---

## ğŸ¯ **Parameter Count Calculation**

```
ENCODER:
â”€â”€â”€â”€â”€â”€â”€â”€
Embedding:              8,000 Ã— 256 = 2,048,000

Bidirectional LSTM Layer 1:
  Forward cell:
    W_ii, W_if, W_ig, W_io:  (256 + 256) Ã— 256 Ã— 4 = 524,288
    Biases:                   256 Ã— 4 = 1,024
  Backward cell:             525,312
  Total Layer 1:             1,050,624

Bidirectional LSTM Layer 2:
  Forward cell (input now 512):  (512 + 256) Ã— 256 Ã— 4 = 786,432 + 1,024
  Backward cell:                 787,456
  Total Layer 2:                 1,574,912

Total Encoder:                   ~4.7M parameters

DECODER:
â”€â”€â”€â”€â”€â”€â”€â”€
Embedding:              10,000 Ã— 256 = 2,560,000

LSTM Layer 1:           (256 + 256) Ã— 256 Ã— 4 + 1,024 = 525,312
LSTM Layer 2:           (256 + 256) Ã— 256 Ã— 4 + 1,024 = 525,312

Attention:
  W_q:                  256 Ã— 256 = 65,536
  W_k:                  512 Ã— 256 = 131,072
  V:                    256 Ã— 1 = 256
  Total:                196,864

Output Dense:           768 Ã— 10,000 = 7,680,000

Total Decoder:          ~11.5M parameters

TOTAL MODEL:            ~16.2M parameters
```

---

## ğŸš¦ **Training Process Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EPOCH LOOP (1 to N)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ TRAINING PHASE
    â”‚   â”‚
    â”‚   â”œâ”€â”€ For each batch in train_loader:
    â”‚   â”‚   â”œâ”€ 1. Get batch (encoder_inputs, decoder_inputs, targets)
    â”‚   â”‚   â”œâ”€ 2. Forward pass through model
    â”‚   â”‚   â”œâ”€ 3. Compute loss and accuracy
    â”‚   â”‚   â”œâ”€ 4. Backward pass (compute gradients)
    â”‚   â”‚   â”œâ”€ 5. Clip gradients
    â”‚   â”‚   â”œâ”€ 6. Optimizer step (update weights)
    â”‚   â”‚   â””â”€ 7. Accumulate metrics
    â”‚   â”‚
    â”‚   â””â”€â”€ Calculate average train loss and accuracy
    â”‚
    â”œâ”€â”€ VALIDATION PHASE
    â”‚   â”‚
    â”‚   â”œâ”€â”€ For each batch in val_loader:
    â”‚   â”‚   â”œâ”€ 1. Forward pass (no gradients)
    â”‚   â”‚   â”œâ”€ 2. Compute loss and accuracy
    â”‚   â”‚   â””â”€ 3. Accumulate metrics
    â”‚   â”‚
    â”‚   â””â”€â”€ Calculate average val loss and accuracy
    â”‚
    â”œâ”€â”€ CHECKPOINT SAVING
    â”‚   â”‚
    â”‚   â””â”€â”€ If val_accuracy > best_val_accuracy:
    â”‚       â”œâ”€ Save model state
    â”‚       â”œâ”€ Save data_dict
    â”‚       â”œâ”€ Save training history
    â”‚       â””â”€ Update best_val_accuracy
    â”‚
    â”œâ”€â”€ LEARNING RATE SCHEDULING
    â”‚   â”‚
    â”‚   â””â”€â”€ Adjust learning rate based on plateau
    â”‚
    â””â”€â”€ TEACHER FORCING ADJUSTMENT
        â”‚
        â””â”€â”€ Decrease teacher_forcing_ratio over epochs
            (e.g., 1.0 â†’ 0.9 â†’ 0.8 â†’ ... â†’ 0.5)
```

---

## ğŸ¬ **Complete Example: Translating "hello world"**

```
INPUT: "hello world"

STEP 1: PREPROCESSING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tokenize:        ['hello', 'world']
To indices:      [45, 231]
Pad:             [45, 231, 0, 0, ..., 0]  (length 20)
To tensor:       torch.tensor([[45, 231, 0, ..., 0]])  # shape [1, 20]

STEP 2: ENCODER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Embed:           [1, 20, 256]
BiLSTM Layer 1:  [1, 20, 512]  (forward + backward concatenated)
BiLSTM Layer 2:  [1, 20, 512]
Final states:    h=[1, 256], c=[1, 256]

STEP 3: DECODER (Iterative)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
t=0:  Input=[SOS] â†’ LSTM â†’ Attention â†’ Output â†’ Predict: "bonjour" (ID=7834)
t=1:  Input=7834  â†’ LSTM â†’ Attention â†’ Output â†’ Predict: "monde"   (ID=4521)
t=2:  Input=4521  â†’ LSTM â†’ Attention â†’ Output â†’ Predict: [EOS]     (ID=2)
Stop!

STEP 4: POST-PROCESSING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Token IDs:       [7834, 4521]
Detokenize:      ["bonjour", "monde"]
Join:            "bonjour monde"

OUTPUT: "bonjour monde"
```

---

## ğŸ“ˆ **Attention Visualization**

```
Source (English):  "hello"   "world"   <PAD>  <PAD>  ...
                      â†‘         â†‘         â†‘      â†‘
Attention Weights:  [0.48]   [0.50]   [0.01]  [0.01] ...
                      â”‚         â”‚         â”‚      â”‚
                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜         â”‚      â”‚
                            â†“             â†“      â†“
Target Word:            "bonjour"
                      (focuses on "hello" and "world")


Source (English):  "hello"   "world"   <PAD>  <PAD>  ...
                      â†‘         â†‘         â†‘      â†‘
Attention Weights:  [0.12]   [0.85]   [0.02]  [0.01] ...
                      â”‚         â”‚         â”‚      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                â†“                â†“
Target Word:              "monde"
                      (focuses mainly on "world")
```

---

## ğŸ”§ **Key Design Decisions**

| Component | Decision | Rationale |
|-----------|----------|-----------|
| **Encoder** | Bidirectional LSTM | Captures context from both directions |
| **Decoder** | Unidirectional LSTM | Can only see past during generation |
| **Attention** | Additive (Bahdanau) | Effective for seq2seq, interpretable |
| **Layers** | Multi-layer (1-3) | Deeper representations, better performance |
| **Dropout** | 0.1-0.2 | Prevents overfitting without hurting performance |
| **Teacher Forcing** | Scheduled (1.0â†’0.5) | Stable training, gradual independence |
| **Optimizer** | Adam | Adaptive learning rates, fast convergence |
| **Loss** | CrossEntropy | Standard for classification tasks |
| **Embedding** | 256-512 dim | Balance between capacity and computation |

---

This architecture represents a **complete, modern seq2seq system** with all the components needed for high-quality neural machine translation! ğŸš€
