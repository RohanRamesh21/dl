"""
Simple test script to verify the improved neural machine translation system
This script tests the key improvements without external dependencies
"""
from train import create_trainer, prepare_training_data, translate_sentence
from data_utils import load_translation_data, prepare_data
import time


def quick_test_improvements():
    """
    Quick test of the key improvements to verify they work correctly
    """
    print("ğŸ§ª Testing Improved NMT System")
    print("=" * 50)
    
    # Load small sample of data
    print("ğŸ“Š Loading sample data...")
    try:
        english, french = load_translation_data('sample_english_french.csv', sample_size=100)
        data_dict = prepare_data(english, french)
        print(f"âœ… Data loaded: {len(english)} samples")
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        return False
    
    # Create model
    print("ğŸ—ï¸  Creating model...")
    try:
        trainer = create_trainer(
            eng_vocab_size=data_dict['eng_vocab_size'],
            fre_vocab_size=data_dict['fre_vocab_size'],
            embedding_dim=64,  # Smaller for testing
            lstm_units=64,     # Smaller for testing  
            learning_rate=0.001,
            device='cpu'
        )
        print("âœ… Model created successfully")
    except Exception as e:
        print(f"âŒ Error creating model: {e}")
        return False
    
    # Test scheduled sampling functionality
    print("ğŸ“ Testing scheduled sampling...")
    try:
        train_data, val_data = prepare_training_data(data_dict)
        
        # Test different teacher forcing ratios
        encoder_inputs = train_data['encoder_inputs'][:2]  # Small batch
        decoder_inputs = train_data['decoder_inputs'][:2] 
        targets = train_data['targets'][:2]
        
        # Test with full teacher forcing (1.0)
        loss_tf_1, acc_tf_1 = trainer.train_step(encoder_inputs, decoder_inputs, targets, teacher_forcing_ratio=1.0)
        print(f"   ğŸ“Š TF=1.0: loss={loss_tf_1:.4f}, acc={acc_tf_1:.4f}")
        
        # Test with partial teacher forcing (0.5)  
        loss_tf_05, acc_tf_05 = trainer.train_step(encoder_inputs, decoder_inputs, targets, teacher_forcing_ratio=0.5)
        print(f"   ğŸ“Š TF=0.5: loss={loss_tf_05:.4f}, acc={acc_tf_05:.4f}")
        
        print("âœ… Scheduled sampling working")
    except Exception as e:
        print(f"âŒ Error testing scheduled sampling: {e}")
        return False
    
    # Test teacher forcing ratio scheduling
    print("ğŸ“… Testing TF ratio scheduling...")
    try:
        # Test ratio calculation
        ratio_epoch_0 = trainer.get_teacher_forcing_ratio(0, 10)
        ratio_epoch_5 = trainer.get_teacher_forcing_ratio(5, 10) 
        ratio_epoch_9 = trainer.get_teacher_forcing_ratio(9, 10)
        
        print(f"   ğŸ“Š Epoch 0/10: TF ratio = {ratio_epoch_0:.3f}")
        print(f"   ğŸ“Š Epoch 5/10: TF ratio = {ratio_epoch_5:.3f}")  
        print(f"   ğŸ“Š Epoch 9/10: TF ratio = {ratio_epoch_9:.3f}")
        
        # Verify decreasing trend
        assert ratio_epoch_0 > ratio_epoch_5 > ratio_epoch_9, "TF ratio should decrease over time"
        print("âœ… TF ratio scheduling working")
    except Exception as e:
        print(f"âŒ Error testing TF scheduling: {e}")
        return False
    
    # Test improved generation
    print("ğŸ¯ Testing improved generation...")
    try:
        # Test basic generation
        test_sentence = "hello"
        translation = translate_sentence(
            trainer, test_sentence, 
            data_dict['eng_tokenizer'], 
            data_dict['fre_tokenizer'],
            data_dict['max_eng_length'],
            device='cpu'
        )
        print(f"   ğŸ“ '{test_sentence}' â†’ '{translation}'")
        
        # Test with beam search
        beam_translation = translate_sentence(
            trainer, test_sentence,
            data_dict['eng_tokenizer'],
            data_dict['fre_tokenizer'], 
            data_dict['max_eng_length'],
            device='cpu',
            use_beam_search=True,
            beam_width=2
        )
        print(f"   ğŸ” Beam search: '{test_sentence}' â†’ '{beam_translation}'")
        
        print("âœ… Generation methods working")
    except Exception as e:
        print(f"âŒ Error testing generation: {e}")
        return False
    
    # Mini training test
    print("ğŸƒ Testing mini training loop...")
    try:
        # Train for just 2 epochs to verify everything works
        history = trainer.fit(
            train_data=train_data,
            val_data=val_data, 
            epochs=2,
            batch_size=16,
            verbose=True
        )
        
        # Check that history contains teacher forcing ratios
        assert 'teacher_forcing_ratio' in history, "History should contain TF ratios"
        assert len(history['teacher_forcing_ratio']) == 2, "Should have TF ratio for each epoch"
        
        print("âœ… Mini training completed successfully")
        print(f"   ğŸ“Š Final train loss: {history['train_loss'][-1]:.4f}")
        print(f"   ğŸ“Š Final val loss: {history['val_loss'][-1]:.4f}")
        print(f"   ğŸ“Š TF ratios: {history['teacher_forcing_ratio']}")
        
    except Exception as e:
        print(f"âŒ Error in mini training: {e}")
        return False
    
    print("\\nğŸ‰ All tests passed!")
    print("ğŸ”§ Key improvements verified:")
    print("   âœ… Scheduled sampling (teacher forcing curriculum)")
    print("   âœ… Teacher forcing ratio scheduling") 
    print("   âœ… Improved decode_step with proper state tracking")
    print("   âœ… Enhanced generation with EOS handling")
    print("   âœ… Beam search decoding")
    print("   âœ… Training loop enhancements")
    
    return True


def test_problematic_case():
    """
    Test the specific problematic case mentioned by the user
    """
    print("\\nğŸ› Testing Problematic Case: 'hello' â†’ weird output")
    print("=" * 50)
    
    # This would be run after training to see if the improvements help
    print("ğŸ’¡ To test this properly, run improved_training.py")
    print("ğŸ’¡ The improvements should help reduce hallucination like:")
    print("   âŒ Before: 'hello' â†’ 'hello elephant how are you'")  
    print("   âœ… After:  'hello' â†’ 'bonjour' (or similar reasonable output)")
    
    print("\\nğŸ”¬ Key reasons for the improvement:")
    print("   1. Fixed decode_step - proper LSTM state tracking")
    print("   2. Scheduled sampling - model learns to handle its own predictions")
    print("   3. Better EOS handling - prevents runaway generation")
    print("   4. Beam search - explores multiple translation paths")


if __name__ == "__main__":
    print("ğŸš€ Starting Comprehensive Test Suite")
    print("=" * 60)
    
    success = quick_test_improvements()
    
    if success:
        test_problematic_case()
        print("\\nğŸ¯ Next Steps:")
        print("   1. Run 'python improved_training.py' for full training")
        print("   2. Compare translations before/after improvements")  
        print("   3. Monitor teacher forcing ratio during training")
        print("   4. Use beam search for better translation quality")
    else:
        print("\\nâŒ Some tests failed. Please check the error messages above.")
    
    print("\\nğŸ“š Summary of Improvements Applied:")
    print("   ğŸ”§ Fixed critical decode_step bug")
    print("   ğŸ“ Added scheduled sampling/teacher forcing curriculum")
    print("   ğŸ” Implemented beam search decoding")
    print("   ğŸ›‘ Better EOS token handling")
    print("   ğŸ“Š Enhanced training monitoring")
    print("   ğŸ› Added debugging capabilities")